---
number: 2
title: Linear Regression and Research Design
day: 17 februar
type: lecture
layout: remark
---

# {{ page.title }} #
## {{ page.day }} ##


---
## Outline

 1. Questions, concerns, and uncertainties
 
 2. Goodness of fit statistics
 
 3. New material
   - Causal inference
   - Multivariate Linear Regression

???

Review confidence intervals briefly

Review normality assumptions
 - Normality of residuals vs. use of the Normal distribution of hypothesis testing

 
---
## BLUE

 - There are many estimators of the population regression equation
 
 - OLS is BLUE due to the Gauss-Markov assumptions
   1. Linearity in parameters
   2. Random sampling
   3. No collinearity
   4. Expected value of errors is zero
   5. Homoskedasticity
   
.footnote[Wooldridge ยง3.5 and p.93]


???

OLS is conventional *for a reason*

Least Squares versus Least Absolute Deviations
 - Both are unbiased estimates of the population regression line
 - OLS is more efficient (small variance)
 - Thus OLS is BLUE


 



---
## Regression goodness of fit

 - Rarely are we only interested in coefficients
 
 - We also want to know if we have a good model

---
## Regression goodness of fit

Four measures:

 - Correlation

 - `\(R^2\)`

 - `\(\sigma\)`

 - F-test

---
.left-column[
### Correlation
]
.right-column[

  - `\(cor(x,y) = \hat{r}_{x,y} = \frac{cov(x,y)}{(n-1)sd(x)sd(y)}\)`

  - Slope `\(\hat{\beta}_1\)` and correlation `\(\hat{r}_{x,y}\)` are simply different scalings of `\(cov(x,y)\)`

  - Correlation tells us how well the bivariate relationship is summarized by a cloud of points

  - Slope tells us the expected change in `\(y\)` given a unit change in `\(x\)`
]
---
.left-column[
### Correlation
### `\(R^2\)`
]
.right-column[

  - "Unexplained variance"

  - Literally the squared correlation coefficient

  - `\(R^2 = cor(x,y)^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\)`

]

---
.left-column[
### Correlation
### Adj. `\(R^2\)`
]
.right-column[

  - `\(R^2\)` increases simply by adding more variables

  - `\(Adj.R^2 = R^2 - (1 - R^2)\frac{k}{n-k-1}\)`, where `\(k\)` is number of regressors

  - Always less than `\(R^2\)`

  - `\(R^2\)` and Adjusted `\(R^2\)` are unitless
]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

 - Standard Error of the Regression (SER) or Root Mean Squared Error (RMSE)

 - How far, on average, are the observed `\(y\)` values from their corresponding fitted values `\(\hat{y}\)`

 - `\(\hat{\sigma} = \sqrt{\frac{RSS}{n-p}}\)`, where `\(p\)` is number of parameters (e.g., `\(\beta_0, \beta_1\)`)

]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

  - Think about how before modelling `\(\bar{y}\)` is our best estimate of `\(y\)` for any `\(x\)`

  - Thus, `\(sd(y)\)` is how far, on average, a given observation is from our estimate of its value

  - After modelling, we have better guesses for each `\(y\)` conditional on `\(x\)` (i.e., the fitted values), so `\(\sigma\)` tells us how far, on average, the observed values our from the fitted values.

  - Thus `\(\hat{\sigma}\)` is never larger than `\(sd(y)\)`

  - `\(\sigma\)` has the same units as our `\(y\)` variable
]

???

Also known by Root Mean Square Error (this is what it is called in Stata)


---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - Omnibus test of whether any of our coefficients differ from zero
  - In a bivariate regression, `\(F=t^2\)`

 - `\(F\)` is a unitless statistic, so it's not directly interpretable

 - It also tests a very uninteresting hypothesis
 
 - But! We can use an F-test to compare "nested" models
]

???

We can compare the residual sum of squares (RSS) from two models and see if including additional covariates reduces RSS

Models have to be "nested" and on the exact same set of observations


---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - F-test for comparing two nested models
   - Reduced model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 \)`
   - Expanded model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots \)`

 - "Reduced" model is nested within the expanded model

 - The F-test comparing the two models tells us if the expanded model significantly reduces RSS
]

???

This is a logical segue into multiple linear regression, which we'll cover next

---
## Goodness of fit in Stata

```
. reg growth lcon

      Source |       SS       df       MS              Number of obs =      44
-------------+------------------------------           F(  1,    42) =    0.09
       Model |  .000038348     1  .000038348           Prob > F      =  0.7615
    Residual |  .017255198    42  .000410838           R-squared     =  0.0022
-------------+------------------------------           Adj R-squared = -0.0215
       Total |  .017293546    43  .000402175           Root MSE      =  .02027

------------------------------------------------------------------------------
      growth |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        lcon |  -.0017819   .0058325    -0.31   0.761    -.0135524    .0099886
       _cons |   .0158988   .0390155     0.41   0.686    -.0628376    .0946353
------------------------------------------------------------------------------
```

---
## Nested model comparison

```
. nestreg: reg growth lcon (lconsq)

+-------------------------------------------------------------+
                Block  Residual                     Change 
Block        F     df        df   Pr > F       R2    in R2 
-------+-----------------------------------------------------
1         0.09      1        42   0.7615   0.0022          
2         7.98      1        41   0.0073   0.1649   0.1626 
+-------------------------------------------------------------+
```


---
template: questions

