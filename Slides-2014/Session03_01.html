---
number: 3
title: Causal Inference and Model Building
day: 17 februar
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

King, Keohane, and Verba Ch.5 (150-207)
Wooldridge 178-208 (150-180) (model fit), 217-248 (182-211) (dummy variables), 293-324 (241-272) (further issues)
Angrist and Pischke Ch.2-3 (11-24, 27-69, 108-110)
Eveland and Scheufele (2000)
Prior (2005)


---
name: outline1

## Outline

 1. Causal inference

--
name: outline2
 2. Model building
 

 
---
class: center
## Exercise

Form pairs

--

Person A: Eveland and Scheufele (2000)

Person B: Prior (2005)

---
class: center
## Exercise

For 90 seconds, think about your assigned article.

How good a job did they do of making a causal inference?

---
class: center
## Exercise

Now, share with your partner.

---
class: center
## Exercise

What did you come up with? 



---
template: outline1

---
## Causal Terminology

 - Unit: A physical object at a particular point in time

 - Treatment: An intervention, whose effects we wish to assess relative to some other (non-)intervention

 - Potential outcomes: The outcome for each unit that we would observe if that unit received each treatment

 - Multiple potential outcomes for each unit, but we only observe one of them

 - Causal effect: The comparisons between the unit-level potential outcomes under each intervention

 - Average causal effect

 
---
## Mill's method of difference

> If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance save one in common, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or cause, or an necessary part of the cause, of the phenomenon.
 
---
## Causal inference

Causal inference is about estimating *what would have happened* in a counterfactual reality

But we can only observe any given unit in one reality!

This is **the fundamental problem of causal inference**


???

Experiments as a benchmark

- Green et al. (2011) from last week


Try to make our data analysis as experiment-like as possible

  - Sometimes we have quasi-experiments
  
  - But usually we just have to build a good model


---
name: questions
class: center, middle

Questions?
  
---
# "Perfect Doctor" #

True potential outcomes (unobservable in reality)

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | 13 | 14 |
| 2 | 6 | 0 |
| 3 | 4 | 1 |
| 4 | 5 | 2 |
| 5 | 6 | 3 |
| 6 | 6 | 1 |
| 7 | 8 | 10 |
| 8 | 8 | 9 |
| *Mean* | *7* | *5* |


???

Pretend we have life expectancy data for 8 patients

We get "God's data", which shows the treatment each patient would have under treatment and control

Clearly, the control is better. On average, patients live longer in the control condition.

---
# "Perfect Doctor" #

How observational data can mislead

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | ? | 14 |
| 2 | 6 | ? |
| 3 | 4 | ? |
| 4 | 5 | ? |
| 5 | 6 | ? |
| 6 | 6 | ? |
| 7 | ? | 10 |
| 8 | ? | 9 |
| *Mean* | *5.4* | *11* |


???

Now we only get to see one potential outcome per patient

But our data are not from an experiment

A "perfect doctor" has assigned each patient to the best treatment for them

Now treatment looks much better than control, even though on average it is worse

(This is basic selection bias)


---
## Causal inference

 - How do we draw inferences when we can't observe every potential outcome for each unit?

 - We focus on average causal effects
   - Compare average outcomes among groups of units
   - Try to make groups comparable by controlling for confounding effects
   
 - SATE vs. PATE
 
???

How do we make groups equal?

 1. Focus on selection bias:
   - What causes units to have a particular value of the treatment/causal variable?
   - Control for those
 
 2. Control for confounding:
   - What things cause both X and Y?
   - Control for those

We can formalize the second one more easily and that's what we'll focus on
   
   
Sample Average Causal Effect (SATE)

Population Average Causal Effect (PATE)

---
template: questions


---
## Causal inference

  When do our statistical inferences based on observational data have *causal* meaning?



  
---
## Five criteria

  1. Relationship
  
  2. Temporal precedence
  
  3. No confounding
  
  4. Plausible mechanism
  
  5. Level of analysis

???

 1. We need a demonstrated relationship between X and Y (e.g., correlation)
 
 2. X needs to precede Y
   - This is easy in experiments due to human intervention
   - What about in cross-sectional data?
   
 3. No reason why we see a relationship other than that X causes Y
 
 4. Does it make sense that X can cause Y (directly or indirectly)
 
 5. No ecological inferences
   - Effect has to be found at the level of the causal effect


How does regression help us with causal inference?

It only helps with items #1 and #2. Everything else is about research design and philosophical assumptions.


---
## Multiple linear regression

Why do we rarely see bivariate regression in published research?

???

The only reason, from a causal inference perspective, to use a multivariate regression model is to control for confounding

Some people - especially Economists and in some, older, PS literature - focus on causal interpretations of all variables in a model

That's usually bad practice

   
---
## Causation in regression

We need to focus on bias in the estimated relationship (slope coefficient) due to:
 
 - Measurement Error
 
 - Post-treatment bias

 - Confounding

---
## An aside: Inefficiencies

 - Random measurement error
 
 - Including irrelevant control variables
 
.footnote[* KKV pp.182-185]
 
???

Inefficiencies make it harder for us to find a relationship by introducing more uncertainty

But they do not necessarily bias our point estimates

We still get the right slope(s), on average, but we just are less certain of the exact value of the slope(s)

---
## Measurement error

What is it?

--

Rather than observing `\(x\)` we observe `\(x\star\)`


???


 1. Constant measurement error not a problem
 
 2. Random measurement error is inefficient but only biasing in the independent variables
 
 3. Systematic measurement error is a big mess and is essentially selection bias


---
## Measurement error

Two kinds of measurement error:

 1. Dependent variable
 
 2. Independent variable(s)
 
Which of these is a problem for us?
 

---
## Errors in Dependent Variables

  - This produces inefficiencies
  
  - Biasing if systematically related to regressor(s)
  
  - Pretty much guaranteed to exist

.footnote[* Wooldridge pp.308-310, KKV pp.158,164-168]

???

But we typically assume that any error is not correlated with regressors

E.g., measurement error in a variable is correlated with the variable itself


---
## Errors in Independent Variables

 - This produces inefficiencies and bias
 
 - It's harder to see an effect (*attenuation*)
 
 - Pretty much guaranteed to exist

.footnote[* Wooldridge pp.310-313,esp. eq 9.33]


???

We have to worry about measurement error both in our causal variable *and* in conditioning variables

If an effect between X and Y is confounded by Z, and Z is measured with error, including Z does not fully break the confounding

Options:
 - We can just live with this (i.e., ignore it)
 - Try to get better data (e.g., multiple measures)
 - Instrumental variables (Session 12)


---
template: questions



---
## Posttreatment bias

 - We usually want to know the **total effect** of a cause
 
 - If we include a mediator, M, of the X -> Y relationship, the coefficient on X:
  
  - Only reflects the **direct** effect
  
  - Excludes the **indirect** effect of X through M

 - So don't control for mediators!
 

???

Is there an example in Eveland and Scheufele? Or in Prior?

Problem is that we can only theorize about possible mediators

---
## Confounding

 - To make a causal claim about a regression coefficient, we need to know that the effect is unconfounded.

 - To do this we can use:
 
   1. Theory
   
   2. Empirical tests of model specification


---
## Common practices

 1. Condition on nothing
   - Estimate a "naive" effect
 
 2. Condition on some variables
   - Theoretically motivated?
   - Data dredging?
 
 3. Condition on observables
   - All observed variables in model
   - All unobserved variables not in model
   

   
---
## Causal models

  - Graphs as expressions of causal theories

  - We can draw out causal theories as a set of directed relationships between variables
  
**Note: This is not Structural Equation Modelling**

---
name: backdoor
## "Backdoor criterion"

How do we know what variables to condition on?

 1. Condition a "fork of mutual dependence" (i.e., confounds)
 
 2. Condition on a complete chain of mediation
 
 3. Do not condition on "colliders" or their descendents
 
 
???

"Condition on" == "Control for" == "Include in model"

Draw examples on board?

Colliders open back door paths

This is expressed in KKV about relevant versus irrelevant omitted variables


 1. Variables that confound the relationship between X and Y
 

---
background-image: url(http://i.imgur.com/cOX02vk.png)

???

This is a simple example of confounding:

C causes both D and Y, so C is a confound we need to control for


---
background-image: url(http://i.imgur.com/0AvbiYc.png)


.footnote[* Wooldridge pp.251-252]

???

Sometimes we can't observe a known confound

One strategy is to condition on a lagged version of the DV

 - The logic is that this controls for unobservables
 
 - But it actually opens up a backdoor path

---
template: backdoor


???

 2. "Smoking kills people" example:
   Smoking -> Tar
   Tar -> Growth of cancer cells
   Cancer -> Morbidity

 3. Colliders are confusing. They open backdoor paths

---
background-image: url(http://i.imgur.com/EvJfY5h.png)


???

A is a collider

We just have to condition on F

No need to condition on G, it's causally irrelevant


---
template: backdoor

Another strategy:

  - Instruments
 
 
.footnote[We'll cover this in Week 12]


---
background-image: url(http://i.imgur.com/EvJfY5h.png)

???

C is an instrument for D

We can talk about the effect of D on Y, local to C

 - **IMPORTANT:** Interactions between variables (see Session 5)
 
---
template: questions



---
## An Aside: Endogeneity

 - We often hear the word **endogeneity**
 
 - It refers to correlation between included variables and the error term
 
 - Causes of endogeneity:
   1. Measurement error in regressors
   
   2. Omitted variables correlated with included regressors
   
   3. Lack of temporal precedence

???

When I say endogeneity, I am almost always referring to the third use of the term

But it also encompasses the first two meanings




---
## Empirical model building
  
 - We can never know the true model
 
 - We can never observe all variables
 
 - We can tell whether and how observed variables should be in the model using residual plots:
   
   1. Plot residuals from a partially specified model against an excluded variable
   
   2. Plot residuals from a fully specified model against an included variable

???

 - Observable versus unobservable variables

 - Observed versus unobserved variables
 
 - We should observe all observable, theoretically important variables

 - **IMPORTANT:** Interactions between variables (see Session 5)
 
---
## Functional form

 - OLS requires the regression model to be "linear in parameters" but the relationship between a given `\(x\)` and `\(y\)` need not be linear
 
 - We can transform variables however we want in order to force a linear relationship
  
  - We want to produce a *linear* Conditional Expectation Function
  
  - To interpret transformations, reverse the transformation to make sense of the coefficient on the original variable scale
 
---
## Functional form

 - We can have multiple versions of the same variable in a model in order to account for nonlinear relationships
  
  - Example 1: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon\)`
  
  - Example 2: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 log(x_1) + \epsilon\)`

 - Identifying appropriate functional form is the same process as identifying appropriate variables to include (theory and residual plots)


---
## RESET test

Can we account for nonlinearities by including power terms?

In Stata:

```
reg growth lcon
ovtest

Ramsey RESET test using powers of the fitted values of growth
       Ho:  model has no omitted variables
                  F(3, 39) =      5.50
                  Prob > F =      0.0030
```

.footnote[Wooldridge pp.296-297]


---
## Correlation with residuals

```
. cor residuals1 lcon lconsq goodgov vlegit
(obs=40)

             | residu~1     lcon   lconsq  goodgov   vlegit
-------------+---------------------------------------------
  residuals1 |   1.0000
        lcon |  -0.1311   1.0000
      lconsq |  -0.1130   0.9985   1.0000
     goodgov |   0.5369   0.1485   0.1603   1.0000
      vlegit |   0.5387  -0.0747  -0.0523   0.3160   1.0000
```



---
## Residual plots

 - Residuals are the unexplained part of the variation in Y
 
 - If there is a relationship between residuals and an included variable, there is endogeneity
 
 - How do we use residuals to build models?
   
   1. Are our included variables included in the correct form?
 
   2. Are there excluded variables that should be in the model?


---
background-image: url(http://i.imgur.com/hD80npW.png)


???

```
reg growth lcon
predict residuals1, resid
graph twoway (scatter residuals1 lconsq) (lfit residuals1 lconsq)
```

---
background-image: url(http://i.imgur.com/6JlE8Ws.png)

???

```
reg growth lcon
predict residuals1, resid
graph twoway scatter residuals1 vlegit
```

---
template: questions


---
class: center
## Exercise

For 90 seconds, think about how well Prior or Eveland and Scheufele did at making a causal inference.

--

Now share with your partner from before.


   
---
## Looking ahead

 - Tomorrow:
   - Regression in Stata
 
 - Next week:
   - More on multivariate regression
   - Review causal inference
   - Standard errors (Kim)
   
 - **First assignment posted on Blackboard and due February 28th**
