---
number: 2
title: Linear Regression and Research Design
day: 10 februar
type: lecture
layout: remark
---

# {{ page.title }} #
## {{ page.day }} ##

???

Wooldridge 20-57 (18-55) (OLS), 64-100 (56-92) (Multivariate OLS and SEs), 110-151 (94-134) (Inference)
Englebert (2000)
Robinson (1950) on ecological inference
Green et al. (2011)


---
class: center
## Next two weeks

Linear regression (OLS)

&

Causal Inference

???

Linear regression is the workhorse procedure of contemporary statistical analysis

Linear regression doesn't guarantee anything about causality, so we need a philosophical perspective on causality to make sense of OLS in causal terms

---
name: outline1

## Outline

 1. What makes a good research question?

--
name: outline2
 2. Uses of regression
 
--
name: outline3
 3. How regression works
 
--
name: outline4
 4. Interpreting regression results
 

---
template: outline1



---
class: center, middle

What makes a good research question?
  

???


Discuss in pairs for two minutes

- KKV's two criteria
  - 1: Politically important
  - 2: Contribute to scientific understanding/literature
- Add:
  - 3: Personally interesting
  - 4: Unresolved

---
## Factors irrelevant to good questions

 1. One's preferred research methods
 
 2. Data availability

 3. Feasibility of the research
 
???

1. Method should never drive question
  - Just because we're focusing on OLS now and regression in general doesn't mean you should think about research questions in those terms
  
2. You will never have the perfect data, so work with what you can get that allows you to gain insights into the question
  
3. Lots of important questions are not feasible, e.g.,
  - observing the origin of the universe, or
  - experimentally manipulating democracy versus authoritarianism)


---
template: outline2

---
## Uses of regression

 1. Description
 
 2. Causal inference
 
 3. Prediction

???

 1. Regression partitions the variation in one variable into variance observable in other variables
   - It tells us about covariance
 2. If we can make philosophical assumptions about the causal relationship between variables, a well-specified regression model tell us about a causal relationship
 3. Regression models (regardless of their descriptive or causal quality) can help us make predictions
   - A regression model helps us make better predictions than just predicting the mean
   - A regression model produces "a line of best fit" (or "a hyperplane of best fit"), which represents the expected value of Y at each combination of values of input variables.
   - For new observations, we can make predictions about what their outcome value might be if we know the values of covariates

---
## Descriptive inferences

 - We want to understand the population of cases
 
 - We cannot look at all of them, so:
 
   1. Draw a *representative* sample
   2. Perform mathematical procedures on sample data
   3. Use assumptions to make inferences about population
   4. Express uncertainty about those inferences
   
???

Example:
1. We want to know what the Danish public thinks about immigration
2. We can't ask everyone
3. We draw a random sample of people and interview them
4. We calculate the sample mean opinion
5. Sampling theory tells us that for a representative sample, the sample mean is an unbiased estimate of the population mean
6. But we cannot say that our sample mean is exactly the population opinion because we sampled, so we express our uncertainty as a function of the variability in our sample data and the size of our sample

Big point: we usually don't care about our sample per se; we only care about it to the extent that it informs population-level inferences

---
## Causal inferences

 - Involve everything that goes into descriptive inference
 
 - Plus, we need some philosophical assumptions
 
 - Plus, we need either randomization or a perfectly specified model
 
 - More on this next week


---
name: questions
class: middle, center

Questions?


---
template: outline3
 

---
## Why did we read Green et al. (2011)?

What was this article about?

What was its analysis?

???

This article demonstrates the simplest possible use of linear regression

 - Mathematically:
   - The independent variable is binary: treatment and control
   - The dependent variable is continuous
   - (A little fanciness in estimation that isn't important for our purposes)

 - Causally: An experiment, so there are almost no concerns about causal inference
   - More on this next week
 

---
## Bivariate regression on an indicator variable

 - Experimental data with two groups
 
 - How do we know if there is an effect of the treatment?
 
 - The mean-difference is the same as the regression slope
 
 - The *t*-statistic from a *t*-test of mean-difference is the same as the *t*-statistic from a *t*-test on an OLS slope for a dummy covariate
 
 
???

MAKE A PLOT OF THIS

Something about slope and `\(\frac{\delta y}{\delta x}\)`

Talk about Green et al. (2011)

It would be great if the world was full of randomized, two-group experiments, but it is not:
  - We often want to look at continuous "treatment" variables
  - Those treatment variables aren't manipulated or exogeneous

We'll focus on the math of moving to continuous independent variables now, then focus on causality next week


---
## Least squares method

 - OLS is a mathematical procedure, not an inferential tool on its own
 
 - We can describe our data using OLS, then we need assumptions to make inferences

 - Population-level regression equation vs. the sample regression equation*
   - Population equation is what we want to know
   - We make inferences about it using the sample equation
   - These should have the same units of analysis

.footnote[Wooldridge pp.21--27]
   
???

Due to law of large numbers, if we have a large enough (representative) sample of the population, our sample estimates will converge on our population parameters (i.e., the estimated regression slope will be the population parameter of interest)


---
## Least squares method

Population equation: `\(Y = \beta_0 + \beta_1 X_1 + \dots\)`

Sample equation: `\(\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \dots\)`

Unit equation: `\(y = \hat{\beta_0} + \hat{\beta_1} x_1 + \dots + u_i\)`

???

The parameters in the population equation are estimated by the statistics in the sample equation

The unit equation is an expression of the observed data in terms of those estimates + residuals



---
## An Aside: Ecological inference

 - Our sample units need to match the unit of analysis of our target population
 
 - Robinson (1950, p.352):
 > Although these studies and score like them depend upon ecological correlations, it is not because their authors are interested in correlations between the properties of areas as such.
 
---
## An Aside: Ecological inference
 - Ecological relationships almost always overstate the relationship between X and Y*
 
 - If we have ecological data for an individual-level research question, we need to either:
   1. Change data (i.e., get individual-level data), or
   2. Change population to which inference is inferred (i.e., change our question)
 
.footnote[* Robinson p.356]


???

Ecological correlation basically mis-weights the individual-level correlations within aggregation areas


---
## Systematic versus unsystematic component of the data

 - Regression line (slope) is the systematic component

 - Linear regression fits the conditional mean of the data (i.e., `\(E[Y|X=x]\)`
  - Conditional Expectation Function*

 - Error term is the deviation of observations from the line (i.e., the unsystematic, unexplained part)
 
.footnote[* Angrist and Pischke ch.3]
  

???

FIGURE TO SHOW THIS

The least squares method minimizes the Residual Sum of Squares (RSS)
 - This is the sum of squared residuals for each observation (vertical distance between observed value and predicted value)
 - No other method can produce a lower RSS

The RSS plus the ESS (explained sum of squares) is how much total variation there are in the data (Total Sum of Squares)

FIGURE TO SHOW THIS
  
  
---
## Systematic versus unsystematic component of the data

 - Consequence: For every level of `\(x\)`, there is only one value of `\(\hat{y}\)`.

   - This may not match the observed values of `\(y\)` for every observation with that value of `\(x\)`

   - Ideally, `\(\hat{y}\)` is the conditional mean of `\(y\)`, given `\(x\)`

   - So we always have some "residual" for each observation



---


???

  - Graphical demonstration of least squares (versus absolute values)
    - Graph of squares
    - Sum areas
    - "Least squares" because no other fit will produce a lower sum of squares


---
## Estimating `\(\hat{\beta}_1\)`

`\(\hat{\beta}_1 = \frac{cov(x,y)}{var(x)}\)`

    - We need variation in X to make this work*
    - Lack of variation in Y is no problem, we'll just get an answer of zero
    - We also need `\(N \ge k\)`, where `\(k\)` is number of parameters to be estimated

  - Multivariate regression problems require matrix algebra**
 
.footnote[
* Wooldridge eq. 2.19 (p.25)

** See session 3a w/Rune and C&T 3.3.2 (pp.82-83).
]


---
## Estimating `\(\hat{\beta}_0\)`

  - Intercept `\(\hat{\beta}_0\)` is just the fitted value of `\(y\)` when `\(x=0\)`
    - Also, `\(\hat{\beta}_0 = \bar{y} - \beta_1\bar{x} \)`

  - This is just simple algebra to calculate once we know `\(\hat{\beta}_1\)`

---
## Is our estimate any good?

 - If estimation works mathematically, and
 - X is measured without error, and
 - The relationship between X and Y is actually linear
 - Then (maybe) yes.
 
 - More on this next week...


---
template: questions
 
---
## Exercise

 - Calculate the slope and intercept for some bivariate data by hand*

 - `\(\hat{\beta}_1 = \frac{cov(x,y)}{var(x)}\)`
   - For each observation, calculate its deviation from `\(\bar{x}\)` and from `\(\bar{x}\)`
   - Numerator: Vector-multiply deviations and sum
   - Denominator: Square deviations and sum
   - Divide numerator by denominator

.footnote[Only works for bivariate case; otherwise use matrix algebra]
  

---
## Exercise

| x | y |
| ---- | ---- |
| 1 | 2 |
| 2 | 5 |
| 3 | 3 |
| 4 | 5 |
| 5 | 4 |
| 6 | 7 |


---
## Exercise

| x | y |
| ---- | ---- |
| 1 | 2 |
| 2 | 5 |
| 3 | 3 |
| 4 | 5 |
| 5 | 4 |
| 6 | 7 |
| `\(\bar{x}=?\)` | `\(\bar{y}=?\)` |

---
## Exercise

| x | y |
| ---- | ---- |
| 1 | 2 |
| 2 | 5 |
| 3 | 3 |
| 4 | 5 |
| 5 | 4 |
| 6 | 7 |
| `\(\bar{x}=3.5\)` | `\(\bar{y}=4.\bar{3}\)` |

---
## Exercise

| x | y | `\(x_i-\bar{x}\)` | `\(y_i-\bar{y}\)` |
| ---- | ---- | --- | --- |
| 1 | 2 | ? | ? |
| 2 | 5 | ? | ? |
| 3 | 3 | ? | ? |
| 4 | 5 | ? | ? |
| 5 | 4 | ? | ? |
| 6 | 7 | ? | ? |
| `\(3.5\)` | `\(4.\bar{3}\)` | | |

???

Regression equation always runs through the point (\((\bar{x}, \bar{y})\)` because everything is in terms of deviations from the mean


---
## Exercise

| x | y | `\((x_i-\bar{x})^2\)` | `\((x_i-\bar{x})*(y_i-\bar{y})\)` |
| ---- | ---- | --- | --- |
| 1 | 2 | ? | ? |
| 2 | 5 | ? | ? |
| 3 | 3 | ? | ? |
| 4 | 5 | ? | ? |
| 5 | 4 | ? | ? |
| 6 | 7 | ? | ? |
| `\(3.5\)` | `\(4.\bar{3}\)` | `\(\Sigma(x_i-\bar{x})^2\)=?` | `\(\Sigma(x_i-\bar{x})(y_i-\bar{y})\)=?` |

`\(\beta_1 = \frac{\Sigma(x_i-\bar{x})(y_i-\bar{y})}{\Sigma(x_i-\bar{x})^2} = ?\)`


???

Correct answer: `\(\beta_1 = 0.6857\)`

`\( \beta_0 = \bar{y} - (\beta_1 * \bar{x}) = 4.\bar{3} - (0.6857 * 3.5) = 1.9\bar{3} \)`


---
template: questions

---
name: residuals

## Fitted values and residuals

 - Our best-fit line doesn't fit our data perfectly

--
 
| x | y | `\(\hat{y_i}\)` | `\(u_i\)` |
| ---- | ---- | --- | --- |
| 1 | 2 | 2.62 | -0.62 |
| 2 | 5 | 3.30 | 1.70 |
| 3 | 3 | 3.99 | -1.01 |
| 4 | 5 | 4.67 | 0.33 |
| 5 | 4 | 5.36 | -1.36 |
| 6 | 7 | 6.05 | 0.95 |
 
---
template: residuals

 - The residuals reflect:
 
   1. Omitted variables (unobserved or unobservable)
  
   2. Measurement error
  
   3. Fundamental randomness

   
???

ADD FIGURE SHOWING RESIDUALS


---
template: outline4



---
## Inference and Hypothesis testing

  - Estimating the slope and intercept only get us point estimates

  - To draw inferences about the population, we need to account for uncertainty in our estimates

  - We do this by generating standard errors (SEs) for the regression estimates

---
## SE Definition

 > The standard error of a sample estimate is the average distance that a sample estimate would be from the population value if we drew lots and lots of separate random samples.
 
???

If we repeated our sampling procedure an infinite number of times, the SE is the average deviation of a sample estimate from the population mean

Standard errors are based on assumptions about the sampling process, not anything about our sample data per se

We never actually sample an infinite number of times

There also may not actually be an infinite number of cases

---
## Regression standard errors

 - `\(var(\hat{\beta_1}) = \frac{\frac{1}{n-2}RSS}{SS_x}\)`

 - `\(s_{\hat{\beta_1}} = \sqrt{\frac{\frac{1}{n-2}RSS}{SS_x}} = \sqrt{\frac{\sigma^2}{SS_x}}\)`

 - `\(s_{\hat{\beta_0}}\)` is more complicated

 - Multivariate regression problems have a more complicated formula


---
## SE Interpretation

  - SE is a ratio of unexplained variance in `\(y\)` (weighted by sample size) and variance in `\(x\)`

  - More variance in `\(x\)` means smaller SE

  - More unexplained variance in `\(y\)` means bigger SE

  - More observations yields smaller numerator in `\(s_{\hat{\beta_1}}\)`, meaning smaller SE

  - The SE has the units of the coefficient (i.e., `\(y\)` per `\(x\)`))

---
## Homoskedasticity and heteroskedasticity

  - The basic SE formula (and the default calculations in Stata) assume homoskedasticity*
  
  - When errors are heteroskedastic, we need "robust" SEs**

.footnote[
* Wooldridge Fig. 2.8-2.9, pp.50--51

** C&T 3.3.4, p.84; Cover this in Session 4
]


---
template: questions


---
## Inferences: Assumptions

- To make a statistical inference about the population slope `\(\beta_1\)` from `\(\hat{\beta_1}\)`, we need to make assumptions

- Normality assumption!!!

  - `\(y|x \sim N(\beta_0 + \beta_1x1 + ..., \sigma^2)\)`


???

We assume in the population, `\(y\)` is Normally distributed with a mean that is a function of our regressor(s) and some *constant* variance `\(\sigma^2\)`

- This allows us to assume the shape of the sampling distribution of our estimated slope
  - Which is that `\(\hat{\beta_1} \sim N(\beta_1, var(\hat{\beta_1}))\)`
  - We don't know `\(var(\beta_1)\)`, so we estimate it by assuming it's `\(var(\hat{\beta_1})\)`
  - We can then further assume a t-shaped sampling distribution for the mean of that distribution


FIGURE SHOWING THE SAMPLING DISTRIBUTION
  
---
## *t*-tests and p-values

- What do we do with all these assumptions? Why do we need them?

- The sampling distribution for `\(\beta_1\)` that we just assumed (partially based on our data), allows us to conduct hypothesis tests

- The kinds of hypotheses we can test are of the form:
  - Bivariate: "`\(x\)` has no effect on `\(y\)`"
  - Multivariate: "accounting for the other variables in the model, `\(x\)` has no effect on the expected value of `\(y\)`"

---
## Hypotheses

  - We often use "no effect" null hypotheses (and this is Stata's default)

  - "No effect" null hypotheses test whether `\(\beta\)` is different from zero

  - To conduct the test, we calculate a *t*-statistic (due to assuming a t-distribution) that is the ratio of estimate over SE:
    - `\(t_{\hat{\beta_1}} = \frac{\hat{\beta_1}}{SE_{\hat{\beta_1}}}\)`

  - We then go back to the *t*-distribution function (which is a function that tells us how likely a given *t*-ratio is)
    - Actually we look at the cumulative distribution function (CDF)

???
t is actually a family of distributions, but with large *n*, t-distribution is basically the normal distribution

---
## Hypotheses

 - We are not restricted to "no effect" null hypotheses
 
 - We can test against any null value
 
 - To do so we simply calculate `\(\frac{\hat{\beta_1} - \alpha}{SE_{\hat{\beta_1}}}\)`, where `\(\alpha\)` is our null hypothesis value
 
 - One such hypothesis would be `\(H_0: \beta_1=1\)`, to test whether there is a one-to-one correspondence between `\(x\)` and `\(y\)`

---
## P-values

 - When *t* is large enough, the cumulative probability of *t*-ratios larger than that value is small
   - i.e., large positive or large negative *t*-ratios put us in the tails of the t-distribution

 - The p-value is the probability of seeing a t-ratio/t-statistic as large or larger than the one we observed in our data
 
---
## P-values

 - A small p-value represents:
 > the probability of a *t*-statistic as extreme as the one we observed, if the null hypothesis was true
 
 - In the regression context:
 > the probability of a slope as large as the one we observed, given the observed variance in the data and that the null hypothesis was true

---
## The p-value is not:

  - The probability that a hypothesis is true or false

  - A reflection of our confidence or certainty about the result

  - The probability that the true slope is in any particular range of values

  - A statement about the importance or substantive size of the effect

---
## Confidence intervals

  - CIs may often be better measures of our uncertainty and as statements about our uncertainty about the true value of a coefficient
  
  - A confidence interval is simply a range, centered on the slope
    - Calculated as a function of `\(SE_{\hat{\beta}}\)` and values derived from the *t*-distribution

  - Confidence intervals and p-values convey the same information
    - But CIs are on the scale of the statistic of interest (and thus the scale of the original data)
  
???

If the confidence interval overlaps zero, we would be unable to reject the null the hypothesis
  
---
## Confidence intervals

  - But, like p-values, CIs are confusing

  - A confidence interval tells us:
  > Were we to repeat our procedure of sampling, analyzing the sample, and calculating a confidence interval *repeatedly* from the population, a fixed percentage of the resulting intervals would include the true population-level slope.
  
  - We cannot say for sure whether the estimated confidence interval *this time* actually includes that true slope

???

- Thus, we can be certain that some percent of the time (the percent being the p-threshold we set in calculating the CI), a given confidence interval includes the true slope

- Need a figure to display this


---
template: questions


---
## Regression goodness of fit

 - Rarely are we only interested in coefficients
 
 - We also want to know if we have a good model

---
## Regression goodness of fit

Four measures:

 - Correlation

 - `\(R^2\)`

 - `\(\sigma\)`

 - F-test

---
.left-column[
### Correlation
]
.right-column[

  - `\(cor(x,y) = \hat{r}_{x,y} = \frac{cov(x,y)}{(n-1)sd(x)sd(y)}\)`

  - Slope `\(\hat{\beta}_1\)` and correlation `\(\hat{r}_{x,y}\)` are simply different scalings of `\(cov(x,y)\)`

  - Correlation tells us how well the bivariate relationship is summarized by a cloud of points

  - Slope tells us the expected change in `\(y\)` given a unit change in `\(x\)`
]
---
.left-column[
### Correlation
### `\(R^2\)`
]
.right-column[

  - "Unexplained variance"

  - Literally the squared correlation coefficient

  - `\(R^2 = cor(x,y)^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\)`

]

---
.left-column[
### Correlation
### Adj. `\(R^2\)`
]
.right-column[

  - `\(R^2\)` increases simply by adding more variables

  - `\(Adj.R^2 = R^2 - (1 - R^2)\frac{k}{n-k-1}\)`, where `\(k\)` is number of regressors

  - Always less than `\(R^2\)`

  - `\(R^2\)` and Adjusted `\(R^2\)` are unitless
]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

 - Standard Error of the Regression (SER) or Root Mean Squared Error (RMSE)

 - How far, on average, are the observed `\(y\)` values from their corresponding fitted values `\(\hat{y}\)`

 - `\(\hat{\sigma} = \sqrt{\frac{RSS}{n-p}}\)`, where `\(p\)` is number of parameters (e.g., `\(\beta_0, \beta_1\)`)

]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

  - Think about how before modelling `\(\bar{y}\)` is our best estimate of `\(y\)` for any `\(x\)`

  - Thus, `\(sd(y)\)` is how far, on average, a given observation is from our estimate of its value

  - After modelling, we have better guesses for each `\(y\)` conditional on `\(x\)` (i.e., the fitted values), so `\(\sigma\)` tells us how far, on average, the observed values our from the fitted values.

  - Thus `\(\hat{\sigma}\)` is never larger than `\(sd(y)\)`

  - `\(\sigma\)` has the same units as our `\(y\)` variable
]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - Omnibus test of whether any of our coefficients differ from zero
  - In a bivariate regression, `\(F=t^2\)`

 - `\(F\)` is a unitless statistic, so it's not directly interpretable

 - It also tests a very uninteresting hypothesis
 
 - But! We can use an F-test to compare "nested" models
]

???

We can compare the residual sum of squares (RSS) from two models and see if including additional covariates reduces RSS

Models have to be "nested" and on the exact same set of observations


---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - F-test for comparing two nested models
   - Reduced model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 \)`
   - Expanded model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots \)`

 - "Reduced" model is nested within the expanded model

 - The F-test comparing the two models tells us if the expanded model significantly reduces RSS
]

???

This is a logical segue into multiple linear regression, which we'll cover next week

---
template: questions


---
## Looking ahead

 - Tomorrow:
   - More Stata basics
   - Graphing
 
 - Next week:
   - Multivariate regression
   - Causal inference
   - Regression in Stata
   
 - **First assignment posted next Monday and due February 28th**

