---
number: 2
title: Linear Regression and Research Design
day: 17 februar
type: lecture
layout: remark
---

# {{ page.title }} #
## {{ page.day }} ##


---
## Outline

 1. Questions, concerns, and uncertainties
 
 2. Goodness of fit statistics
 
 3. New material
   - Causal inference
   - Multivariate Linear Regression

???

Review confidence intervals briefly

Review normality assumptions
 - Normality of residuals vs. use of the Normal distribution of hypothesis testing

---
## Regression goodness of fit

 - Rarely are we only interested in coefficients
 
 - We also want to know if we have a good model

---
## Regression goodness of fit

Four measures:

 - Correlation

 - `\(R^2\)`

 - `\(\sigma\)`

 - F-test

---
.left-column[
### Correlation
]
.right-column[

  - `\(cor(x,y) = \hat{r}_{x,y} = \frac{cov(x,y)}{(n-1)sd(x)sd(y)}\)`

  - Slope `\(\hat{\beta}_1\)` and correlation `\(\hat{r}_{x,y}\)` are simply different scalings of `\(cov(x,y)\)`

  - Correlation tells us how well the bivariate relationship is summarized by a cloud of points

  - Slope tells us the expected change in `\(y\)` given a unit change in `\(x\)`
]
---
.left-column[
### Correlation
### `\(R^2\)`
]
.right-column[

  - "Unexplained variance"

  - Literally the squared correlation coefficient

  - `\(R^2 = cor(x,y)^2 = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\)`

]

---
.left-column[
### Correlation
### Adj. `\(R^2\)`
]
.right-column[

  - `\(R^2\)` increases simply by adding more variables

  - `\(Adj.R^2 = R^2 - (1 - R^2)\frac{k}{n-k-1}\)`, where `\(k\)` is number of regressors

  - Always less than `\(R^2\)`

  - `\(R^2\)` and Adjusted `\(R^2\)` are unitless
]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

 - Standard Error of the Regression (SER) or Root Mean Squared Error (RMSE)

 - How far, on average, are the observed `\(y\)` values from their corresponding fitted values `\(\hat{y}\)`

 - `\(\hat{\sigma} = \sqrt{\frac{RSS}{n-p}}\)`, where `\(p\)` is number of parameters (e.g., `\(\beta_0, \beta_1\)`)

]

---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
]
.right-column[

  - Think about how before modelling `\(\bar{y}\)` is our best estimate of `\(y\)` for any `\(x\)`

  - Thus, `\(sd(y)\)` is how far, on average, a given observation is from our estimate of its value

  - After modelling, we have better guesses for each `\(y\)` conditional on `\(x\)` (i.e., the fitted values), so `\(\sigma\)` tells us how far, on average, the observed values our from the fitted values.

  - Thus `\(\hat{\sigma}\)` is never larger than `\(sd(y)\)`

  - `\(\sigma\)` has the same units as our `\(y\)` variable
]

???

Also known by Root Mean Square Error (this is what it is called in Stata)


---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - Omnibus test of whether any of our coefficients differ from zero
  - In a bivariate regression, `\(F=t^2\)`

 - `\(F\)` is a unitless statistic, so it's not directly interpretable

 - It also tests a very uninteresting hypothesis
 
 - But! We can use an F-test to compare "nested" models
]

???

We can compare the residual sum of squares (RSS) from two models and see if including additional covariates reduces RSS

Models have to be "nested" and on the exact same set of observations


---
.left-column[
### Correlation
### `\(R^2\)`
### `\(\sigma\)`
### F-test
]
.right-column[

 - F-test for comparing two nested models
   - Reduced model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 \)`
   - Expanded model: `\( \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots \)`

 - "Reduced" model is nested within the expanded model

 - The F-test comparing the two models tells us if the expanded model significantly reduces RSS
]

???

This is a logical segue into multiple linear regression, which we'll cover next

---
template: questions

