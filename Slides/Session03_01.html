---
number: 3
title: Causal Inference and Model Building
day: 17 februar
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

King, Keohane, and Verba Ch.5 (150-207)
Wooldridge 178-208 (150-180) (model fit), 217-248 (182-211) (dummy variables), 293-324 (241-272) (further issues)
Angrist and Pischke Ch.2-3 (11-24, 27-69, 108-110)
Eveland and Scheufele (2000)
Prior (2005)


---
name: outline1

## Outline

 1. Causal inference

--
name: outline2
 2. Model building
 

--
name: outline3
 3. Reporting and interpreting regression results
 

 
---
class: center
## Exercise

Think about the Eveland and Scheufele (2000) article.

What is their core argument?

---
class: center
## Exercise

For 90 seconds, think about:

How good a job did they do of making a causal inference?

---
class: center
## Exercise

Now, share with the person sitting next to you.

---
class: center
## Exercise

What did you come up with? 



---
template: outline1

---
## Causal Terminology

 - Unit: A physical object at a particular point in time

 - Treatment: An intervention, whose effects we wish to assess relative to some other (non-)intervention

 - Potential outcomes: The outcome for each unit that we would observe if that unit received each treatment

 - Multiple potential outcomes for each unit, but we only observe one of them

 - Causal effect: The comparisons between the unit-level potential outcomes under each intervention

 - Average causal effect

 
---
## Mill's method of difference

> If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance save one in common, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or cause, or an necessary part of the cause, of the phenomenon.
 
---
## Causal inference

Causal inference is about estimating *what would have happened* in a counterfactual reality

But we can only observe any given unit in one reality!

This is **the fundamental problem of causal inference**


???

Experiments as a benchmark

- Green et al. (2011) from last week


Try to make our data analysis as experiment-like as possible

  - Sometimes we have quasi-experiments
  
  - But usually we just have to build a good model


---
name: questions
class: center

Questions?
  
---
# "Perfect Doctor" #

True potential outcomes (unobservable in reality)

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | 13 | 14 |
| 2 | 6 | 0 |
| 3 | 4 | 1 |
| 4 | 5 | 2 |
| 5 | 6 | 3 |
| 6 | 6 | 1 |
| 7 | 8 | 10 |
| 8 | 8 | 9 |
| *Mean* | *7* | *5* |


???

Pretend we have life expectancy data for 8 patients

We get "God's data", which shows the treatment each patient would have under treatment and control

Clearly, the control is better. On average, patients live longer in the control condition.

---
# "Perfect Doctor" #

How observational data can mislead

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | ? | 14 |
| 2 | 6 | ? |
| 3 | 4 | ? |
| 4 | 5 | ? |
| 5 | 6 | ? |
| 6 | 6 | ? |
| 7 | ? | 10 |
| 8 | ? | 9 |
| *Mean* | *5.4* | *11* |


???

Now we only get to see one potential outcome per patient

But our data are not from an experiment

A "perfect doctor" has assigned each patient to the best treatment for them

Now treatment looks much better than control, even though on average it is worse

(This is basic selection bias)


---
## Causal inference

 - How do we draw inferences when we can't observe every potential outcome for each unit?

 - We focus on average causal effects
   - Compare average outcomes among groups of units
   - Try to make groups comparable by controlling for confounding effects
   
 - SATE vs. PATE
 
???


Sample Average Causal Effect (SATE)

Population Average Causal Effect (PATE)

---
template: questions


---
## Causal inference

  When do our statistical inferences based on observational data have *causal* meaning?



  
---
## Five criteria

  1. Relationship
  
  2. Temporal precedence
  
  3. No confounding
  
  4. Plausible mechanism
  
  5. Level of analysis

???

How does regression help us with causal inference?

It only helps with items #1 and #2. Everything else is about research design and philosophical assumptions.


---
## Multiple linear regression

Why do we rarely see bivariate regression in published research?

???

The only reason, from a causal inference perspective, to use a multivariate regression model is to control for confounding

Some people - especially Economists and in some, older, PS literature - focus on causal interpretations of all variables in a model

That's usually bad practice

---
## Causation in regression

We need to focus on bias in the estimated relationship (slope coefficient) due to:
 
 - Measurement Error
 
 - Post-treatment bias

 - Confounding

---
## An aside: Inefficiencies

 - Measurement error in the independent variables
 
 - Including irrelevant control variables
 
.footnote[* KKV pp.182-185]
 
???

Inefficiencies make it harder for us to find a relationship by introducing more uncertainty

But they do not bias our point estimates

We still get the right slope(s), on average, but we just are less certain of the exact value of the slope(s)

---
## Measurement error

Two kinds of measurement error:

 1. Dependent variable
 
 2. Independent variable(s)
 
Which of these is a problem for us?
 
???



---
## Errors in Dependent Variables


.footnote[* Wooldridge pp.308-310, KKV pp.158,164-168]



---
## Errors in Independent Variables


.footnote[* Wooldridge pp.310-313]


???


---
template: questions



---
## Posttreatment bias

 - We usually want to know the **total effect** of a cause
 
 - If we include a mediator, M, of the X -> Y relationship, the coefficient on X:
  - Only reflects the **direct** effect
  - Excludes the **indirect** effect of X through M

 - So don't control for mediators!
 

???

Is there an example in Eveland and Scheufele? Or in Prior?

Problem is that we can only theorize about possible mediators

---
## Confounding

 - To make a causal claim about a regression coefficient, we need to know that the effect is unconfounded.

 - To do this we can use:
 
   1. Theory
   
   2. Empirical tests of model specification


---
## Common practices

 1. Condition on nothing
   - Estimate a "naive" effect
 
 2. Condition on some variables
   - Theoretically motivated?
   - Data dredging?
 
 3. Condition on observables
   - All observed variables in model
   - All unobserved variables not in model
   

   
---
## Causal models

  - Graphs as expressions of causal theories


---
name: backdoor
## "Backdoor criterion"

How do we know what variables to condition on?

 1. Condition a "fork of mutual dependence" (i.e., confounds)
 
 2. Condition on a complete chain of mediation
 
 3. Do not condition on "colliders" or their descendents
 
 
???

"Condition on" == "Control for" == "Include in model"

Draw examples on board?

Colliders open back door paths

This is expressed in KKV about relevant versus irrelevant omitted variables


 1. Variables that confound the relationship between X and Y
 

---
background-image: url(http://i.imgur.com/cOX02vk.png)

???

This is a simple example of confounding:

C causes both D and Y, so C is a confound we need to control for


---
background-image: url(http://i.imgur.com/0AvbiYc.png)


.footnote[ADD CITATION TO WOOLDRIDGE]

???

Sometimes we can't observe a known confound

One strategy is to condition on a lagged version of the DV

 - The logic is that this controls for unobservables
 
 - But it actually opens up a backdoor path

---
template: backdoor


 2. "Smoking kills people" example:
   Smoking -> Tar
   Tar -> Growth of cancer cells
   Cancer -> Morbidity

 3. Colliders are confusing. They open backdoor paths

---
background-image: url(http://i.imgur.com/EvJfY5h.png)


???

A is a collider

We just have to condition on F

No need to condition on G, it's causally irrelevant


---
template: backdoor

 4. Instruments
 
 
.footnote[We'll cover this in Week 12]


---
background-image: url(http://i.imgur.com/EvJfY5h.png)

???

C is an instrument for D

We can talk about the effect of D on Y, local to C


---
template: questions



---
## An Aside: Endogeneity

 - We often hear the word **endogeneity**
 
 - It refers to correlation between included variables and the error term
 
 - Causes of endogeneity:
   1. Measurement error in regressors
   
   2. Omitted variables correlated with included regressors
   
   3. Lack of temporal precedence

???

When I say endogeneity, I am almost always referring to the third use of the term

But it also encompasses the first two meanings




---
## Empirical model building
  
 - We can never know the true model
 
 - We can never observe all variables
 
 - We can tell whether and how observed variables should be in the model using residual plots:
   
   1. Plot residuals from a partially specified model against an excluded variable
   
   2. Plot residuals from a fully specified model against an included variable

???

 - Observable versus unobservable variables

 - Observed versus unobserved variables
 
 - We should observe all observable, theoretically important variables

 - **IMPORTANT:** Interactions between variables (see Session 5)
 
---
## Functional form

 - OLS requires the regression model to be "linear in parameters" but the relationship between a given `\(x\)` and `\(y\)` need not be linear
 
 - We can transform variables however we want in order to force a linear relationship
  - We want to produce a *linear* Conditional Expectation Function
  - To interpret transformations, reverse the transformation to make sense of the coefficient on the original variable scale
 
---
## Functional form

 - We can have multiple versions of the same variable in a model in order to account for nonlinear relationships
  
  - Example 1: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon\)`
  
  - Example 2: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 log(x_1) + \epsilon\)`

 - Identifying appropriate functional form is the same process as identifying appropriate variables to include (theory and residual plots)


---
## RESET test

Can we account for nonlinearities by including power terms?

In Stata:

```
reg growth lcon
ovtest
```

.footnote[Wooldridge pp.296-297]


---
## Residual plots


???

NEED A FIGURE TO SHOW THIS



---
template: questions


---
class: center
## Exercise

For 90 seconds, think about how well Eveland and Scheufele did at making a causal inference.

--

Now share with the person sitting next to you.





---
## Influential observations

 - Some observations have high "leverage," having a dramatic influence on the estimated coefficient(s) in a model
 
 - Influential observations are essentially outliers
 
 - They might be multi-dimensional outliers and thus hard to see
 
 - We can measure leverage directly, but often use Cook's Distance
 
???

Cook's Distance incorporates information about residuals in addition to leverage
 
---
## Cook's Distance
 
 - Cook's Distance is calculated by "jacknifing"
   - Re-running the model *n*-times, leaving out one observation each time
 
 - Run the model with all the data, then compare those coefficients to the jacknifed models
   - Cook's Distance is high when the differences between those coefficients are large
   
   - `\(\frac{n}{4}\)` is conventional threshold for "high" Cook's Distance
 
---
## Cook's Distance

 - We can then drop those observations if we're worried about how they influence the model

 - We lose representativeness

 - Possibly gain a better insight into the shape of the relationships in the reduced dataset
  

???

The issue of dropping observations is something we'll revisit

How do we deal with outliers and missing data?

Trade-off representativeness and quality/precision of inference about a reduced population
  
---
## Influential Observations in Stata

```
use EnglebertPRQ2000.dta

reg growth lcon lconsq
predict newvar, cooksd
summ newvar

* biggest Cook's distance
scalar cookmax = r(max)
tab country if cookmax = newvar

* see Cook's distance by country
tab country, summarize(newvar) nost nofr

* leverage plot
lvr2plot, mlabel(country)
```


---
template: questions


---
template: outline3

---
## Interpretation of coefficients

 - In bivariate regression, `\(\beta_1\)` is just the slope (line of best fit)
 
 - In multivariate regression, interpreting coefficients is more complex
   - Imagine a *hyperplane*

 - Accounting for the variation attributable to other variables in the model, what is the effect of `\(x\)` on `\(y\)`
    - Holding all other variables at their means, what is the effect of x on y

???

Instead of thinking of a line of best fit, we have to think about a *hyperplane of best fit* that spans across all of the independent variables


---

???

NEED A FIGURE SHOWING A HYPERPLANE




---
## Interpretation of coefficients

  - Our interpretations are sample-level interpretations
    - We need to have a representative sample to make population-level inferences
    - But we rarely have truly representative samples
  
  - Uncertainty means coefficients in model are unlikely to be exactly the population-level ("true") effects
    - Better not to think about the point estimates (slopes) but instead the interval

???

Sometimes we have populations

Even random samples are not always representative
 - Bad sampling procedures
 - Nonresponse
 
    
---
## Comparing coefficients
 
 - The size of the effect depends on the units of `\(x\)`
    - Think about money: kr versus 1000s of kr; coefficient is larger for 1000s
    - p-value is not an indicator of effect size
 
 - It's not easy to compare substantive size of coefficients unless regressors have comparable units
 
 - Otherwise it's apples and oranges comparisons
 
???

Don't say an effect is "highly significant"
 
---
## Comparing standardized coefficients

 - Sometimes people "standardize" variables (so `\(x\)` becomes `\(x\ast\)`)
 
 - A standard deviation change in `\(x\)` equals a unit-change in `\(x\ast\)`
 
 - The coefficient `\(\beta\ast\)` is interpreted as standard deviation changes in `\(y\)` per standard deviation change in `\(x\)`
 
 - "Comparing standardized apples to standardized oranges"
 
.footnote[[http://gking.harvard.edu/files/mist.pdf](http://gking.harvard.edu/files/mist.pdf)]
    
---
## Types of variables

 - Interval
 
 - Indicator
 
 - Categorical or ordinal
 
---
.left-column[
### Interval
]
.right-column[

 - Change in `\(y\)` for a unit-change in `\(x\)`

]

---
.left-column[
### Interval
### Indicator
]
.right-column[

 - Bivariate regression is equivalent to a *t*-test
 
 - Like the experimental example from last week
]

---
.left-column[
### Interval
### Indicator
### Categorical
]
.right-column[

 - Is it ordinal?

 - Treat as interval
   - Be cautious about linearity and the Conditional Expectation Function
   - How categorical variables are coded can affect the estimate since the scale is imposed by the researcher
 
 - Convert to indicators*
   - A series of pairwise comparisons to a baseline condition

.footnote[* Wooldridge pp.225-230]

]   



---
template: questions

  

---
## Reporting regression results

 - What is normally reported from a regression model?
 
   1. Coefficients
   
   2. Standard errors (or *t*-statistics)
   
   3. Significance stars
   
   4. `\(R^2\)` or Adjusted-`\(R^2\)` 

---
## Reporting regression results

 - What should we report from a regression model?
 
   - Coefficients
   
   - Confidence intervals or standard errors
   
   - Measures of model fit
     1. RMSE (`\(\hat{\sigma}\)`)
     2. Adjusted-`\(R^2\)`
     
   - Model-specific sample size (*n*)

   
---
## Looking ahead

 - Tomorrow:
   - Regression in Stata
 
 - Next week:
   - More on multivariate regression
   - Review causal inference
   - Standard errors (Kim)
   
 - **First assignment posted on Blackboard and due February 28th**
