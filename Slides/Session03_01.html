---
number: 3
title: Causal Inference and Model Building
day: 17 februar
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

King, Keohane, and Verba Ch.5 (150-207)
Wooldridge 178-208 (150-180) (model fit), 217-248 (182-211) (dummy variables), 293-324 (241-272) (further issues)
Angrist and Pischke Ch.2-3 (11-24, 27-69, 108-110)
Eveland and Scheufele (2000)
Prior (2005)


---
name: outline1

## Outline

 1. Causal inference

--
name: outline2
 2. Model building
 
--
name: outline3
 3. Reporting and interpreting regression results
 

 
---
class: center
## Exercise

Form pairs

--

Person A: Eveland and Scheufele (2000)

Person B: Prior (2005)

---
class: center
## Exercise

For 90 seconds, think about your assigned article.

How good a job did they do of making a causal inference?

---
class: center
## Exercise

Now, share with your partner.

---
class: center
## Exercise

What did you come up with? 



---
template: outline1

---
## Causal Terminology

 - Unit: A physical object at a particular point in time

 - Treatment: An intervention, whose effects we wish to assess relative to some other (non-)intervention

 - Potential outcomes: The outcome for each unit that we would observe if that unit received each treatment

 - Multiple potential outcomes for each unit, but we only observe one of them

 - Causal effect: The comparisons between the unit-level potential outcomes under each intervention

 - Average causal effect

 
---
## Mill's method of difference

> If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance save one in common, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or cause, or an necessary part of the cause, of the phenomenon.
 
---
## Causal inference

Causal inference is about estimating *what would have happened* in a counterfactual reality

But we can only observe any given unit in one reality!

This is **the fundamental problem of causal inference**


???

Experiments as a benchmark

- Green et al. (2011) from last week


Try to make our data analysis as experiment-like as possible

  - Sometimes we have quasi-experiments
  
  - But usually we just have to build a good model


---
name: questions
class: center, middle

Questions?
  
---
# "Perfect Doctor" #

True potential outcomes (unobservable in reality)

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | 13 | 14 |
| 2 | 6 | 0 |
| 3 | 4 | 1 |
| 4 | 5 | 2 |
| 5 | 6 | 3 |
| 6 | 6 | 1 |
| 7 | 8 | 10 |
| 8 | 8 | 9 |
| *Mean* | *7* | *5* |


???

Pretend we have life expectancy data for 8 patients

We get "God's data", which shows the treatment each patient would have under treatment and control

Clearly, the control is better. On average, patients live longer in the control condition.

---
# "Perfect Doctor" #

How observational data can mislead

| Unit | Y(0) | Y(1) |
| ---- | ---- | ---- |
| 1 | ? | 14 |
| 2 | 6 | ? |
| 3 | 4 | ? |
| 4 | 5 | ? |
| 5 | 6 | ? |
| 6 | 6 | ? |
| 7 | ? | 10 |
| 8 | ? | 9 |
| *Mean* | *5.4* | *11* |


???

Now we only get to see one potential outcome per patient

But our data are not from an experiment

A "perfect doctor" has assigned each patient to the best treatment for them

Now treatment looks much better than control, even though on average it is worse

(This is basic selection bias)


---
## Causal inference

 - How do we draw inferences when we can't observe every potential outcome for each unit?

 - We focus on average causal effects
   - Compare average outcomes among groups of units
   - Try to make groups comparable by controlling for confounding effects
   
 - SATE vs. PATE
 
???

How do we make groups equal?

 1. Focus on selection bias:
   - What causes units to have a particular value of the treatment/causal variable?
   - Control for those
 
 2. Control for confounding:
   - What things cause both X and Y?
   - Control for those

We can formalize the second one more easily and that's what we'll focus on
   
   
Sample Average Causal Effect (SATE)

Population Average Causal Effect (PATE)

---
template: questions


---
## Causal inference

  When do our statistical inferences based on observational data have *causal* meaning?



  
---
## Five criteria

  1. Relationship
  
  2. Temporal precedence
  
  3. No confounding
  
  4. Plausible mechanism
  
  5. Level of analysis

???

 1. We need a demonstrated relationship between X and Y (e.g., correlation)
 
 2. X needs to precede Y
   - This is easy in experiments due to human intervention
   - What about in cross-sectional data?
   
 3. No reason why we see a relationship other than that X causes Y
 
 4. Does it make sense that X can cause Y (directly or indirectly)
 
 5. No ecological inferences
   - Effect has to be found at the level of the causal effect


How does regression help us with causal inference?

It only helps with items #1 and #2. Everything else is about research design and philosophical assumptions.


---
## Multiple linear regression

Why do we rarely see bivariate regression in published research?

???

The only reason, from a causal inference perspective, to use a multivariate regression model is to control for confounding

Some people - especially Economists and in some, older, PS literature - focus on causal interpretations of all variables in a model

That's usually bad practice

   
---
## Causation in regression

We need to focus on bias in the estimated relationship (slope coefficient) due to:
 
 - Measurement Error
 
 - Post-treatment bias

 - Confounding

---
## An aside: Inefficiencies

 - Random measurement error
 
 - Including irrelevant control variables
 
.footnote[* KKV pp.182-185]
 
???

Inefficiencies make it harder for us to find a relationship by introducing more uncertainty

But they do not necessarily bias our point estimates

We still get the right slope(s), on average, but we just are less certain of the exact value of the slope(s)

---
## Measurement error

Two kinds of measurement error:

 1. Dependent variable
 
 2. Independent variable(s)
 
Which of these is a problem for us?
 
???


 1. Constant measurement error not a problem
 
 2. Random measurement error is inefficient but only biasing in the independent variables
 
 3. Systematic measurement error is a big mess and is essentially selection bias


---
## Errors in Dependent Variables

  - This produces inefficiencies
  
  - Biasing if systematically related to regressor(s)
  
  - Pretty much guaranteed to exist

.footnote[* Wooldridge pp.308-310, KKV pp.158,164-168]



---
## Errors in Independent Variables

 - This produces inefficiencies and bias
 
 - It's harder to see an effect (*attenuation*)
 
 - Pretty much guaranteed to exist

.footnote[* Wooldridge pp.310-313]


???

We have to worry about measurement error both in our causal variable *and* in conditioning variables

If an effect between X and Y is confounded by Z, and Z is measured with error, including Z does not fully break the confounding

Options:
 - We can just live with this (i.e., ignore it)
 - Try to get better data (e.g., multiple measures)
 - Instrumental variables


---
template: questions



---
## Posttreatment bias

 - We usually want to know the **total effect** of a cause
 
 - If we include a mediator, M, of the X -> Y relationship, the coefficient on X:
  
  - Only reflects the **direct** effect
  
  - Excludes the **indirect** effect of X through M

 - So don't control for mediators!
 

???

Is there an example in Eveland and Scheufele? Or in Prior?

Problem is that we can only theorize about possible mediators

---
## Confounding

 - To make a causal claim about a regression coefficient, we need to know that the effect is unconfounded.

 - To do this we can use:
 
   1. Theory
   
   2. Empirical tests of model specification


---
## Common practices

 1. Condition on nothing
   - Estimate a "naive" effect
 
 2. Condition on some variables
   - Theoretically motivated?
   - Data dredging?
 
 3. Condition on observables
   - All observed variables in model
   - All unobserved variables not in model
   

   
---
## Causal models

  - Graphs as expressions of causal theories

  - We can draw out causal theories as a set of directed relationships between variables
  
**Note: This is not Structural Equation Modelling**

---
name: backdoor
## "Backdoor criterion"

How do we know what variables to condition on?

 1. Condition a "fork of mutual dependence" (i.e., confounds)
 
 2. Condition on a complete chain of mediation
 
 3. Do not condition on "colliders" or their descendents
 
 
???

"Condition on" == "Control for" == "Include in model"

Draw examples on board?

Colliders open back door paths

This is expressed in KKV about relevant versus irrelevant omitted variables


 1. Variables that confound the relationship between X and Y
 

---
background-image: url(http://i.imgur.com/cOX02vk.png)

???

This is a simple example of confounding:

C causes both D and Y, so C is a confound we need to control for


---
background-image: url(http://i.imgur.com/0AvbiYc.png)


.footnote[* Wooldridge pp.251-252]

???

Sometimes we can't observe a known confound

One strategy is to condition on a lagged version of the DV

 - The logic is that this controls for unobservables
 
 - But it actually opens up a backdoor path

---
template: backdoor


???

 2. "Smoking kills people" example:
   Smoking -> Tar
   Tar -> Growth of cancer cells
   Cancer -> Morbidity

 3. Colliders are confusing. They open backdoor paths

---
background-image: url(http://i.imgur.com/EvJfY5h.png)


???

A is a collider

We just have to condition on F

No need to condition on G, it's causally irrelevant


---
template: backdoor

Another strategy:

  - Instruments
 
 
.footnote[We'll cover this in Week 12]


---
background-image: url(http://i.imgur.com/EvJfY5h.png)

???

C is an instrument for D

We can talk about the effect of D on Y, local to C

 - **IMPORTANT:** Interactions between variables (see Session 5)
 
---
template: questions



---
## An Aside: Endogeneity

 - We often hear the word **endogeneity**
 
 - It refers to correlation between included variables and the error term
 
 - Causes of endogeneity:
   1. Measurement error in regressors
   
   2. Omitted variables correlated with included regressors
   
   3. Lack of temporal precedence

???

When I say endogeneity, I am almost always referring to the third use of the term

But it also encompasses the first two meanings




---
## Empirical model building
  
 - We can never know the true model
 
 - We can never observe all variables
 
 - We can tell whether and how observed variables should be in the model using residual plots:
   
   1. Plot residuals from a partially specified model against an excluded variable
   
   2. Plot residuals from a fully specified model against an included variable

???

 - Observable versus unobservable variables

 - Observed versus unobserved variables
 
 - We should observe all observable, theoretically important variables

 - **IMPORTANT:** Interactions between variables (see Session 5)
 
---
## Functional form

 - OLS requires the regression model to be "linear in parameters" but the relationship between a given `\(x\)` and `\(y\)` need not be linear
 
 - We can transform variables however we want in order to force a linear relationship
  
  - We want to produce a *linear* Conditional Expectation Function
  
  - To interpret transformations, reverse the transformation to make sense of the coefficient on the original variable scale
 
---
## Functional form

 - We can have multiple versions of the same variable in a model in order to account for nonlinear relationships
  
  - Example 1: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \epsilon\)`
  
  - Example 2: `\(\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 log(x_1) + \epsilon\)`

 - Identifying appropriate functional form is the same process as identifying appropriate variables to include (theory and residual plots)


---
## RESET test

Can we account for nonlinearities by including power terms?

In Stata:

```
reg growth lcon
ovtest

Ramsey RESET test using powers of the fitted values of growth
       Ho:  model has no omitted variables
                  F(3, 39) =      5.50
                  Prob > F =      0.0030
```

.footnote[Wooldridge pp.296-297]


---
## Correlation with residuals

```
. cor residuals1 lcon lconsq goodgov vlegit
(obs=40)

             | residu~1     lcon   lconsq  goodgov   vlegit
-------------+---------------------------------------------
  residuals1 |   1.0000
        lcon |  -0.1311   1.0000
      lconsq |  -0.1130   0.9985   1.0000
     goodgov |   0.5369   0.1485   0.1603   1.0000
      vlegit |   0.5387  -0.0747  -0.0523   0.3160   1.0000
```



---
## Residual plots

 - Residuals are the unexplained part of the variation in Y
 
 - If there is a relationship between residuals and an included variable, there is endogeneity
 
 - How do we use residuals to build models?
   
   1. Are our included variables included in the correct form?
 
   2. Are there excluded variables that should be in the model?


---
## Functional Form


![http://i.imgur.com/hD80npW.png](http://i.imgur.com/hD80npW.png)


???

```
reg growth lcon
predict residuals1, resid
graph twoway (scatter residuals1 lconsq) (lfit residuals1 lconsq)
```

---
## Omitted variable

![http://i.imgur.com/6JlE8Ws.png](http://i.imgur.com/6JlE8Ws.png)

???

```
reg growth lcon
predict residuals1, resid
graph twoway scatter residuals1 vlegit
```

---
template: questions


---
class: center
## Exercise

For 90 seconds, think about how well Prior or Eveland and Scheufele did at making a causal inference.

--

Now share with your partner from before.



---
## Influential observations

 - Some observations have high "leverage," having a dramatic influence on the estimated coefficient(s) in a model
 
 - Influential observations are essentially outliers
 
 - They might be multi-dimensional outliers and thus hard to see
 
 - We can measure leverage directly, but often use Cook's Distance
 
???

Cook's Distance incorporates information about residuals in addition to leverage
 
---
## Cook's Distance
 
 - Cook's Distance is calculated by "jacknifing"
   - Re-running the model *n*-times, leaving out one observation each time
 
 - Run the model with all the data, then compare those coefficients to the jacknifed models
   - Cook's Distance is high when the differences between those coefficients are large
   
   - `\(\frac{n}{4}\)` is conventional threshold for "high" Cook's Distance
 
---
## Cook's Distance

 - We can then drop those observations if we're worried about how they influence the model

 - We lose representativeness

 - Possibly gain a better insight into the shape of the relationships in the reduced dataset
  

???

The issue of dropping observations is something we'll revisit

How do we deal with outliers and missing data?

Trade-off representativeness and quality/precision of inference about a reduced population
  
---
## Influential Observations in Stata

```
use EnglebertPRQ2000.dta

reg growth lcon lconsq
predict newvar, cooksd
summ newvar

* biggest Cook's distance
scalar cookmax = r(max)
tab country if newvar == cookmax, summarize(newvar) nost nofr

* see Cook's distance by country
tab country, summarize(newvar) nost nofr
list country newvar, table

* leverage plot
lvr2plot, mlabel(country)
```

---
## Influential Observations in Stata

```
. tab country if newvar == cookmax, summarize(newvar) nost nofr

                     | Summary of
                     |  Cook's D
     Name of country |        Mean
---------------------+------------
            ETHIOPIA |   .55600208
---------------------+------------
               Total |   .55600208
```

---
## Influential Observations in Stata

```
. tab country, summarize(newvar) nost nofr

                     | Summary of
                     |  Cook's D
     Name of country |        Mean
---------------------+------------
              ANGOLA |   .02927462
               BENIN |   .00005987
            BOTSWANA |   .09608144
        BURKINA FASO |   .00326944
             BURUNDI |   .00053345
            CAMEROON |   .00671317
...
```


---
## Influential Observations in Stata

```
 list country newvar, table

     +---------------------------------+
     |              country     newvar |
     |---------------------------------|
....
     |---------------------------------|
 41. |           CAPE VERDE   .0748542 |
 42. |             BOTSWANA   .0960814 |
 43. |           SEYCHELLES   .2439945 |
 44. |             ETHIOPIA   .5560021 |
 45. |    SAO TOME-PRINCIPE          . |
     |---------------------------------|
 46. |              ERITREA          . |
 47. |              NAMIBIA          . |
 48. |              MAYOTTE          . |
 49. |              REUNION          . |
 50. |    EQUATORIAL GUINEA          . |
     +---------------------------------+
```


---
## Influential Observations in Stata

![http://i.imgur.com/yBRHjSP.png](http://i.imgur.com/yBRHjSP.png)


---
template: questions


---
template: outline3

---
## Interpretation of coefficients

 - In bivariate regression, `\(\beta_1\)` is just the slope (line of best fit)
 
 - In multivariate regression, interpreting coefficients is more complex
   - Imagine a *hyperplane*

 - Accounting for the variation attributable to other variables in the model, what is the effect of `\(x\)` on `\(y\)`
    - Holding all other variables at their means, what is the effect of x on y

???

Instead of thinking of a line of best fit, we have to think about a *hyperplane of best fit* that spans across all of the independent variables


---

???

NEED A FIGURE SHOWING A HYPERPLANE




---
## Interpretation of coefficients

  - Our interpretations are sample-level interpretations
    - We need to have a representative sample to make population-level inferences
    - But we rarely have truly representative samples
  
  - Uncertainty means coefficients in model are unlikely to be exactly the population-level ("true") effects
    - Better not to think about the point estimates (slopes) but instead the interval
???

Sometimes we have populations

Even random samples are not always representative
 - Bad sampling procedures
 - Nonresponse
 
   
---
## Comparing coefficients
 
 - The size of the effect depends on the units of `\(x\)`
    - Think about money: kr versus 1000s of kr; coefficient is larger for 1000s
    - p-value is not an indicator of effect size
 
 - It's not easy to compare substantive size of coefficients unless regressors have comparable units
 
 - Otherwise it's apples and oranges comparisons
 
???

Don't say an effect is "highly significant"
 
---
## Comparing standardized coefficients

 - Sometimes people "standardize" variables (so `\(x\)` becomes `\(x^{\ast}\)`)
 
 - A standard deviation change in `\(x\)` equals a unit-change in `\(x^{\ast}\)`
 
 - The coefficient `\(\beta^{\ast}\)` is interpreted as standard deviation changes in `\(y\)` per standard deviation change in `\(x\)`
 
 - "Comparing standardized apples to standardized oranges"
 
.footnote[[http://gking.harvard.edu/files/mist.pdf](http://gking.harvard.edu/files/mist.pdf)]
    
---
## Types of variables

 - Interval
 
 - Indicator
 
 - Categorical or ordinal
 
---
.left-column[
### Interval
]
.right-column[

 - Change in `\(y\)` for a unit-change in `\(x\)`

]

---
.left-column[
### Interval
### Indicator
]
.right-column[

 - Bivariate regression is equivalent to a *t*-test
 
 - Like the experimental example from last week
]

---
.left-column[
### Interval
### Indicator
### Categorical
]
.right-column[

 - Is it ordinal?

 - Treat as interval
   - Be cautious about linearity and the Conditional Expectation Function
   - How categorical variables are coded can affect the estimate since the scale is imposed by the researcher
 
 - Convert to indicators*
   - A series of pairwise comparisons to a baseline condition

.footnote[* Wooldridge pp.225-230]

]   

---
template: questions

  

---
## Reporting regression results

 - What is normally reported from a regression model?
 
   1. Coefficients
   
   2. Standard errors (or *t*-statistics)
   
   3. Significance stars
   
   4. `\(R^2\)` or Adjusted-`\(R^2\)` 

---
## Reporting regression results

 - What should we report from a regression model?
 
   - Coefficients
   
   - Confidence intervals or standard errors
   
   - Measures of model fit
     1. RMSE (`\(\hat{\sigma}\)`)
     2. Adjusted-`\(R^2\)`
     
   - Model-specific sample size (*n*)

   
---
## Looking ahead

 - Tomorrow:
   - Regression in Stata
 
 - Next week:
   - More on multivariate regression
   - Review causal inference
   - Standard errors (Kim)
   
 - **First assignment posted on Blackboard and due February 28th**
