---
number: 3
title: Causal Inference and Model Building
day: 24 februar
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #


---
name: outline
## Outline

 - Interpretation of Coefficients
 
 - Influential Observations (briefly)

 - Causal Inference Review

???

Questions about assignment?

Stata blogs: Take a look b/c there are some good hints about how to use esttab to produce nice output.

Anette posted a particularly complete example that's worth taking a look at.

---
## Interpretation of coefficients

 - In bivariate regression, `\(\beta_1\)` is just the slope (line of best fit)
 
 - In multivariate regression, interpreting coefficients is more complex
   - Imagine a *hyperplane*

 - Accounting for the variation attributable to other variables in the model, what is the effect of `\(x\)` on `\(y\)`

???

Instead of thinking of a line of best fit, we have to think about a *hyperplane of best fit* that spans across all of the independent variables


---
background-image: url(http://www.mathworks.com/matlabcentral/fx_files/7239/1/planenormvec.jpg)


---
## Interpretation of coefficients

Two things to keep in mind:

 - Effect `\(\beta_1\)` is constant across values of X1
 
 - Effect is *also* constant across values of other covariates
 
 - This all changes when we have:
   1. Interaction terms
   2. Nonlinear covariates (e.g., `\(x^2\)`)
   2. Models other than OLS

---
## Interpretation of coefficients

  - Our interpretations are sample-level interpretations
    - We need to have a representative sample to make population-level inferences
    - But we rarely have truly representative samples
  
  - Uncertainty means coefficients in model are unlikely to be exactly the population-level ("true") effects
    - Better not to think about the point estimates (slopes) but instead the interval
???

Sometimes we have populations

Even random samples are not always representative
 - Bad sampling procedures
 - Nonresponse
 
   

---
## Exercise

Work through the worksheet in groups 

---
## Types of variables

 - Interval
 
 - Indicator
 
 - Categorical or ordinal
 
---
.left-column[
### Interval
]
.right-column[

 - Change in `\(y\)` for a unit-change in `\(x\)`

]

---
.left-column[
### Interval
### Indicator
]
.right-column[

 - Bivariate regression is equivalent to a *t*-test
 
 - Like the experimental example from last week
]

---
.left-column[
### Interval
### Indicator
### Categorical
]
.right-column[

 - Is it ordinal?

 - Treat as interval
   - Be cautious about linearity and the Conditional Expectation Function
   - How categorical variables are coded can affect the estimate since the scale is imposed by the researcher
 
 - Convert to indicators*
   - A series of pairwise comparisons to a baseline condition

.footnote[* Wooldridge pp.225-230]

]   

---
## Comparing coefficients
 
 - The size of `\(\beta\)` depends on the units of `\(x\)`
    - Think about money: kr versus 1000s of kr; coefficient is smaller for 1000s
    - p-value is not an indicator of effect size
 
 - It's not easy to compare substantive size of coefficients unless regressors have comparable units
 
 - Otherwise it's apples and oranges comparisons
 
???

Don't say an effect is "highly significant"
 
---
## Comparing standardized coefficients

 - Sometimes people "standardize" variables (so `\(x\)` becomes `\(x^{\ast}\)`)
 
 - A standard deviation change in `\(x\)` equals a unit-change in `\(x^{\ast}\)`
 
 - The coefficient `\(\beta^{\ast}\)` is interpreted as standard deviation changes in `\(y\)` per standard deviation change in `\(x\)`
 
 - "Comparing standardized apples to standardized oranges"
 
.footnote[[http://gking.harvard.edu/files/mist.pdf](http://gking.harvard.edu/files/mist.pdf)]


---
name: questions
class: center, middle

Questions?


---
## Influential observations

 - Some observations have high "leverage," having a dramatic influence on the estimated coefficient(s) in a model
 
 - Influential observations are essentially outliers
 
 - They might be multi-dimensional outliers and thus hard to see
 
 - We can measure leverage directly, but often use Cook's Distance
 
???

Cook's Distance incorporates information about residuals in addition to leverage
 
---
## Cook's Distance
 
 - Cook's Distance is calculated by "jacknifing"
   - Re-running the model *n*-times, leaving out one observation each time
 
 - Run the model with all the data, then compare those coefficients to the jacknifed models
   - Cook's Distance is high when the differences between those coefficients are large
   
   - `\(\frac{n}{4}\)` is conventional threshold for "high" Cook's Distance
 
---
## Cook's Distance

 - We can then drop those observations if we're worried about how they influence the model

 - We lose representativeness

 - Possibly gain a better insight into the shape of the relationships in the reduced dataset
  

???

The issue of dropping observations is something we'll revisit

How do we deal with outliers and missing data?

Trade-off representativeness and quality/precision of inference about a reduced population
  
---
## Influential Observations in Stata

```
use EnglebertPRQ2000.dta

reg growth lcon lconsq
predict newvar, cooksd
summ newvar

* biggest Cook's distance
scalar cookmax = r(max)
tab country if newvar == cookmax, summarize(newvar) nost nofr

* see Cook's distance by country
tab country, summarize(newvar) nost nofr
list country newvar, table

* leverage plot
lvr2plot, mlabel(country)
```

---
## Influential Observations in Stata

```
. tab country if newvar == cookmax, summarize(newvar) nost nofr

                     | Summary of
                     |  Cook's D
     Name of country |        Mean
---------------------+------------
            ETHIOPIA |   .55600208
---------------------+------------
               Total |   .55600208
```

---
## Influential Observations in Stata

```
. tab country, summarize(newvar) nost nofr

                     | Summary of
                     |  Cook's D
     Name of country |        Mean
---------------------+------------
              ANGOLA |   .02927462
               BENIN |   .00005987
            BOTSWANA |   .09608144
        BURKINA FASO |   .00326944
             BURUNDI |   .00053345
            CAMEROON |   .00671317
...
```


---
## Influential Observations in Stata

```
 list country newvar, table

     +---------------------------------+
     |              country     newvar |
     |---------------------------------|
....
     |---------------------------------|
 41. |           CAPE VERDE   .0748542 |
 42. |             BOTSWANA   .0960814 |
 43. |           SEYCHELLES   .2439945 |
 44. |             ETHIOPIA   .5560021 |
 45. |    SAO TOME-PRINCIPE          . |
     |---------------------------------|
 46. |              ERITREA          . |
 47. |              NAMIBIA          . |
 48. |              MAYOTTE          . |
 49. |              REUNION          . |
 50. |    EQUATORIAL GUINEA          . |
     +---------------------------------+
```


---
background-image: url(http://i.imgur.com/yBRHjSP.png)


---
template: questions



---
## Causal inference

What criteria do we need to satisfy causal inference?


???

 1. Effect
 2. Temporal precedence
 3. Nonconfounding
 4. Mechanism
 5. Level of analysis
 
Temporal precedence is at the level of causation, not measurement

What do I mean by that?

I'm interested in the effect of smoking on cancer. So, I look at a sample of patients (some who already have cancer and some who do not) and ask them how much they smoked in the past. Does this satisfy temporal order?
