---
number: 11
title: Matching Methods
day: 14 april
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

Rubin (2001)
Ho, Imai, King, and Stuart (2007)
Angrist and Pischke (69-91)
Kam and Palmer (2008)
Henderson and Chatfield (2011)


---
name: outline

## Outline

 - Experiments and Matching
 
 - Matching Techniques
 
 - Assessing Matching

---
## Why do I like matching?

 - Forces the researcher to look at the data

 - Matching and regression are equivalent
 
 - Encourages good research practice

???

Everything we talk about today applies to all research

 1. Matching focuses our attention on whether we've eliminated confounding due to *observed* factors; this is something we need to do anytime we're trying to make a causal claim

 2. Regression and matching seem really different, but they're ultimately both about eliminating confounding
 
 3. Matching is about designing an observational study. You don't need any of the data to design your model. You decide what variables you're going to match on (control for) before seeing the data.
   - Additionally, you never look at the outcome data until you have your matched sample in hand
 
---
## Experiments

 - If we can run an experiment, we should
 
 - If we cannot run an experiment, we can try to emulate one


???


---
## Does smoking cause cancer?

Experiment

 - Randomly assign individuals to:
   - Treatment: Smoking
   - Control: No smoking
   
 - Observe cancer rates in both groups
 
 - Causal effect is difference in cancer rates
 
???

How does this study satisfy the five criteria for causal inference?

---
## Five causal criteria

  1. Relationship
  
  2. Temporal precedence
  
  3. No confounding
  
  4. Plausible mechanism
  
  5. Level of analysis

???

But what's wrong with the experimental study?

  

---
## Does smoking cause cancer?

Observational study

 - Observe treatment status
 
 - Observe cancer rates in both groups
 
 - **Naive causal effect** is difference in cancer rates
 
???

How does this study satisfy the five criteria for causal inference?


What's wrong with it?

What factors predict smoking?


Naive because it shows naivete. It ignores confounding: all of the factors that affect selection into smoking.


 
---
## Experiments

 - Experiments work because of the **ignorability** of treatment assignment
 
 - Randomization of treatment assignment means no factor (no variable) causes a unit to receive the treatment rather than the control

 - Matching is about **conditionality ignorability** of confounding factors
 
???



---
## Matching

 - Treatment assignment is **conditionally ignorable** if, in our sample data, no factor predicts treatment assignment
 
 - We achieve this by:
  
   - Selecting units that share pretreatment characteristics
   - Selectively discarding observations that do not **match**

 - Conditional ignorability lets us make a **causal** claim about the effect of treatment

???

---
name: questions
class: center, middle

# Questions?


---
background-image: url(http://i.imgur.com/EvJfY5h.png)

???

We always use the same causal logic

We want to block "backdoor paths"

In matching, we do that by focusing on variables that cause selection into categories of the treatment variable

We only need to condition to close backdoor paths, but we can additionally condition on anything that causes selection


---
## Considerations

 - Matched sample is a nonrandom subsample of our original data
 
 - We sacrifice representativeness for the ability to make a clearer causal inference
 
 - Matching may not be possible (no common covariate support)
 
 - Matching is highly covariate-dependent


???

Without covariate support, we shouldn't do regression either


Choice of covariates can have a big impact on results. More on this in a minute.


---
template: questions


---
template: outline


---
## How do we match?

 - Define treatment and control groups
 
 - Observe variables that affect treatment selection
   - *We can never know all of these!*

 - Construct a matched sample (matching and pruning)
 
 - Analyze matched sample as we see fit
   
???

We can do this with more than two groups (more than just T and C), but the math gets complicated

Easier to do it with Exact matching


---
## An Example


```
Treatment units:        T T T T T T T T T T T T T T T

Control units:    C C C C C C C C C C C C C C

                <--------------- W ------------------->
```

???

MATCHING ON ONE COVARIATE

Age affects whether someone smokes

If age also affects whether someone gets cancer (older people more likely to get cancer) then the effect of age will confound the effect of smoking

If we look only at area of common covariate support, then smokers and non-smokers do not differ on age

Age becomes ignorable in our assessment of the causal effect of smoking on cancer



---
template: questions



---
## Three big choices

 1. Which covariates do we match on?
 
 2. How do we do the matching?

 3. How do we know when units are sufficiently matched?
 
---
## Which covariates do we match on?

 - Common practice is called **matching on observables**
 
 - Go back to graphs

---
background-image: url(http://i.imgur.com/EvJfY5h.png)

---
name: methods

## How do we do the matching?

 1. Exact matching
 
 2. Coarsened exact matching
 
 3. Nearest-neighbor matching
 
 4. Propensity score matching
 
 5. Propensity score sub-classification
 

---
## Exact Matching

 - Benefits:
 
   - Groups are automatically balanced
 
 - Challenges:
 
   - Choice of covariates
   - Sparsity can be problematic
   - Problem of dimensionality
   
---
## An Example: Exact matching

```
Treatment units:        T   T T       T T     T T T

Control units:    C C   C C     C C C C C C       C

                <--------------- W ------------------->
```

???

Exact matching we discard anyone who isn't a perfect match

---
## An Example: Coarsened exact matching

```
Treatment units:        T   T T       T T     T T T

Control units:    C C   C C     C C C C C C       C

                <--------------- W ------------------->
                <   Low  |    Medium     |     High   >
```

???

We coarsen our covariate and create matches within each band


---
template: questions



---
## Problem of Dimensionality

 - Problem: As number and levels of covariates increase, too many levels to match at
 
 - E.g.:
 
   - If we need to match on gender:
     - There are 2 levels (male and female)
     - We need 4 observations (2 smokers and 2 non-smokers)
   
   - If we also match on 'having a university degree':
     - We have 4 levels:
       - Male with degree and Female with degree
       - Male w/o degree and Female w/o degree
     - We need 8 observations (4 smokers and 4 non-smokers)

---
## Activity: How many levels do we need if we match on...

 1. Gender
--

 2. and having children or not
--

 3. and level of education (no high school, high school, BA, MA+)
--

 4. and age (in years)
--

 5. and monthly income


---
## Problem of Dimensionality

 - Problem: As number and levels of covariates increase, too many levels to match at
 
 - Solutions:
   
   1. Coarsened exact matching (CEM)
   2. Nearest-neighbor matching
     - We won't discuss this in detail
   3. Propensity score matching/subclassification



---
template: questions



---
## Nearest neighbor matching

```
Treatment units:        T   T T       T T     T T T

Control units:    C C   C C     C C C C C C       C

                <--------------- W ------------------->
```

???

We pick a treated observation and we find the closest matching control unit

When everyone is matched we discard the remaining data

This is sensitive to order of choosing observations


---
## Propensity score matching

 1. Gather all covariates to match on
 
 2. Estimate a binary outcome GLM predicting treatment assignment
 
 3. Match cases based on propensity score (a single covariate)
 
   - Predicted probability of being in treatment group, or
   - Logit (log-odds)

???

We can either match exactly, use nearest-neighbor on the propensity score, or subclassify (like in one-variable CEM)

---
## Propensity score matching

 - Benefits:
 
   - Solves problem of dimensionality
   - Assess balance on only one dimension
   
 - Challenges:
 
   - Choice of covariates
   - Model dependence (logit/probit)
   - Decision about 1-to-1 or 1-to-k or k-to-k matching
   - Assessing balance
   
---
## Estimating propensity scores in Stata

```
. logit smoker age gender income married educ

Logistic regression            Number of obs =   1013
                               LR chi2(5)    =  74.54
                               Prob > chi2   = 0.0000
Log likelihood =   -444.263    Pseudo R2     = 0.0774

-----------------------------------------------------
      smoker |      Coef.   Std. Err.      z    P>|z|
-------------+---------------------------------------
         age |  -.0157091    .004411    -3.56   0.000
      gender |  -.2836812   .1707935    -1.66   0.097
      income |  -.0049973   .0026905    -1.86   0.063
     married |   .4239441   .1209327     3.51   0.000
        educ |  -.4888199   .0787023    -6.21   0.000
       _cons |   .3100827   .4036072     0.77   0.442
-----------------------------------------------------
```

---
## Estimating propensity scores in Stata

```
quietly logit smoker age gender income married educ

* predict pscore on log-odds-ratio scale
predict pslogodds, xb

* predict pscore on probability scale
predict psprob, pr

. cor pslogodds psprob
             | pslogo~s   psprob
-------------+------------------
   pslogodds |   1.0000
      psprob |   0.9370   1.0000
```

???

It's conventional to match on the log-odds scale, but you can also use the probability scale

---
background-image: url(http://i.imgur.com/t07ahvy.png)

---
## Estimating propensity scores in Stata

```
* create pscore subclasses
egen pssubclass = cut(pslogodds), group(5)

tab pssubclass

 pssubclass |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        202       19.94       19.94
          1 |        203       20.04       39.98
          2 |        202       19.94       59.92
          3 |        203       20.04       79.96
          4 |        203       20.04      100.00
------------+-----------------------------------
      Total |      1,013      100.00
```

---
template: questions


---
## Activity

 - Divide into groups
 - Summarize one section to share with class

From Kam and Palmer:

   1) Argument (K&P pp.614-617)
   
   2) Methods (K&P pp.617-622)
   
   3) Results (K&P pp.622-626)

From Henderson and Chatfield:

   4) Method (H&C pp.649-652)
   
   5) Results (H&C pp.653-655)

 
???

Divide class into groups of ~3

Have each pair take a part of the two articles

 1. Which result should we trust more? Why?

 2. What is the critical issue in Kam and Palmer's approach?

---
template: outline

---
## How do we know when units are sufficiently matched?

 - Common covariate support
 
 - Covariate (mean) balance
 
 - Identical covariate distributions

???

Because there are unobserved covariates, we can never really know

This is also a problem for regression; there are always omitted variables

 1. We get common covariate support in any matching exercise
 
 2. Mean balance is something we have to test (exact matching ignores it)
 
   - We can do t-test, but do we need exactly identical means?
   - Do we look at the propensity score or individual covariates?
 
 3. Identical distributions

   - Is it necessary?
   - How do we test?


---
## Aside: Exact matching

 - In **exact matching** we do not need to check balance
 
 - All variables that we match on are, by definition, balanced

---
background-image: url(http://i.imgur.com/w8lkqL9.png)

???

Age


 1. Common Covariate Support? Yes
 
 2. Mean balance? No
 
 3. Same distributions? No

---
background-image: url(http://i.imgur.com/IC6VpFL.png)

???

Education

 1. Common Covariate Support? No
 
 2. Mean balance? No
 
 3. Same distributions? No

---
background-image: url(http://i.imgur.com/uk6NOxV.png)

???

Children in HH

 1. Common Covariate Support? Yes
 
 2. Mean balance? Yes
 
 3. Same distributions? Yes

 
---

```
           |        GENDER:
     Smoke |      Male     Female |     Total
-----------+----------------------+----------
       Yes |        96         89 |       185 
           |     20.65      16.24 |     18.26 
-----------+----------------------+----------
        No |       369        459 |       828 
           |     79.35      83.76 |     81.74 
-----------+----------------------+----------
     Total |       465        548 |     1,013 
           |    100.00     100.00 |    100.00
```

???

Gender

 1. Common Covariate Support? Yes
 
 2. Mean balance? No
 
 3. Same distributions? No


---
template: questions


---
## Analysis

 - Matching is **preprocessing**
 
 - We still need to estimate a causal effect, using any method:

   - Difference-in-means
   - Weighted difference
   - Regression

???

Up to this point, we haven't discussed outcomes.

The philosophy of matching is that you settle on matched sample before looking at outcome data.

This way you aren't picking a model based on the causal effect estimate that comes out of the procedure.




---
## Effect estimation in Stata

```
. teffects psmatch (outcome) (smoker age gender income married educ)

Treatment-effects estimation                    Number of obs      =      1013
Estimator      : propensity-score matching      Matches: requested =         1
Outcome model  : matching                                      min =         1
Treatment model: logit                                         max =         2
------------------------------------------------------------------------------
             |              AI Robust
     outcome |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
ATE          |
      smoker |
 (Yes vs 0)  |   .0853899   .0277538     3.08   0.002     .0309936    .1397863
------------------------------------------------------------------------------
```


???

STATA INTEGRATES MATCHING AND EFFECT ESTIMATION

IN LAB, WE'LL DO MATCHING BY HAND, THEN USE STATA'S BUILT-IN MATCHING PROCEDURES




---
## Preview

Tomorrow:

 - Matching in Stata
 
Next week:

 - Instrumental Variables (IV regression)
 
Week after next:

 - Missing data
 - Multiple imputation

???

IV focuses on estimating a Local Average Treatment Effect (LATE) by finding some real-world randomization to "instrument" for the causal variable

So-called "natural experiments", "regression discontinuity designs", and "interrupted time series" are all basically instrumental variables designs
