---
number: 11
title: Matching Methods
day: 14 april
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

Rubin (2001)
Ho, Imai, King, and Stuart (2007)
Angrist and Pischke (69-91)
Kam and Palmer (2008)
Henderson and Chatfield (2011)


---
name: outline

## Outline

 - Experiments and Matching
 
 - Matching Techniques
 
 - Assessing Matching

---
## Why do I like matching?

 - Forces the researcher to look at the data

 - Matching and regression are equivalent

???

Everything we talk about today applies to all research

Matching focuses our attention on whether we've eliminated confounding due to *observed* factors; this is something we need to do anytime we're trying to make a causal claim

Regression and matching seem really different, but they're ultimately both about eliminating confounding
 
 
---
## Experiments

 - If we can run an experiment, we should
 
 - If we cannot run an experiment, we can try to emulate one


???


---
## Does smoking cause cancer?

Experiment

 - Randomly assign individuals to:
   - Treatment: Smoking
   - Control: No smoking
   
 - Observe cancer rates in both groups
 
 - Causal effect is difference in cancer rates
 
???

How does this study satisfy the five criteria for causal inference?

---
## Five causal criteria

  1. Relationship
  
  2. Temporal precedence
  
  3. No confounding
  
  4. Plausible mechanism
  
  5. Level of analysis

???

But what's wrong with the experimental study?

  

---
## Does smoking cause cancer?

Observational study

 - Observe treatment status
 
 - Observe cancer rates in both groups
 
 - **Naive causal effect** is difference in cancer rates
 
???

How does this study satisfy the five criteria for causal inference?


What's wrong with it?

What factors predict smoking?


Naive because it shows naivete. It ignores confounding: all of the factors that affect selection into smoking.


 
---
## Experiments

 - Experiments work because of the **ignorability** of treatment assignment
 
 - Randomization of treatment assignment means no factor (no variable) causes a unit to receive the treatment rather than the control

 - Matching is about **conditionality ignorability** of confounding factors
 
???



---
## Matching

 - Treatment assignment is **conditionally ignorable** if, in our sample data, no factor predicts treatment assignment
 
 - We achieve this by:
  
   - Selecting units that share pretreatment characteristics
   - Selectively discarding observations that do not **match**

 - Conditional ignorability lets us make a **causal** claim about the effect of treatment

???


---
background-image: url(http://i.imgur.com/EvJfY5h.png)

???

We always use the same causal logic

We want to block "backdoor paths"

In matching, we do that by focusing on variables that cause selection into categories of the treatment variable

We only need to condition to close backdoor paths, but we can additionally condition on anything that causes selection


---
## Considerations

 - Matched sample is a nonrandom subsample of our original data
 
 - We sacrifice representativeness for the ability to make a clearer causal inference
 
 - Matching may not be possible (no common covariate support)
 
 - Matching is highly covariate-dependent


???

Without covariate support, we shouldn't do regression either


Choice of covariates can have a big impact on results. More on this in a minute.

 
---
template: outline


---
## How do we match?

 - Define treatment and control groups
 
 - Observe variables that affect treatment selection
   - *We can never know all of these!*

 - Construct a matched sample (matching and pruning)
 
 - Analyze matched sample as we see fit
   
???

We can do this with more than two groups (more than just T and C), but the math gets complicated

Easier to do it with Exact matching


---
## An Example


```
        T T T T T T T T T T T T T T T

  C C C C C C C C C C C C C C

<--------------- W ------------------->
```

???

MATCHING ON ONE COVARIATE

Age affects whether someone smokes

If age also affects whether someone gets cancer (older people more likely to get cancer) then the effect of age will confound the effect of smoking

If we look only at area of common covariate support, then smokers and non-smokers do not differ on age

Age becomes ignorable in our assessment of the causal effect of smoking on cancer


---
## Three big choices

 1. Which covariates do we match on?
 
 2. How do we do the matching?

 3. How do we know when units are sufficiently matched?
 
---
## Which covariates do we match on?

 - Common practice is called **matching on observables**
 
 - Go back to graphs

---
background-image: url(http://i.imgur.com/EvJfY5h.png)

---
name: methods

## How do we do the matching?

 1. Exact matching
 
 2. Coarsened exact matching
 
 3. Nearest-neighbor matching
 
 4. Propensity score matching
 
 5. Propensity score sub-classification
 

---
## Exact Matching

 - Benefits:
 
   - Groups are automatically balanced
 
 - Challenges:
 
   - Choice of covariates
   - Sparsity can be problematic
   - Problem of dimensionality
   
---
## An Example: Exact matching

```
        T   T T       T T     T T T

  C C   C C     C C C C C C       C

<--------------- W ------------------->
```

???

Exact matching we discard anyone who isn't a perfect match

---
## An Example: Coarsened exact matching

```
        T   T T       T T     T T T

  C C   C C     C C C C C C       C

<   Low  |    Medium     |     High   >
  
<--------------- W ------------------->
```

???

We coarsen our covariate and create matches within each band


---
## Problem of Dimensionality

 - Problem: As number and levels of covariates increase, too many levels to match at
 
 - Solutions:
   
   1. Coarsened exact matching (CEM)
   2. Nearest-neighbor matching (We won't discuss this)
   3. Propensity score matching/subclassification


---
## Nearest neighbor matching

```
        T   T T       T T     T T T

  C C   C C     C C C C C C       C

<--------------- W ------------------->
```

???

We pick a treated observation and we find the closest matching control unit

When everyone is matched we discard the remaining data

This is sensitive to order of choosing observations


---
## Propensity score matching

 1. Gather all covariates to match on
 
 2. Estimate a binary outcome GLM predicting treatment assignment
 
 3. Match cases based on propensity score (a single covariate)
 
   - Predicted probability of being in treatment group, or
   - Logit (log-odds)

???

We can either match exactly, use nearest-neighbor on the propensity score, or subclassify (like in CEM)

---
## Propensity score matching

 - Benefits:
 
   - Solves problem of dimensionality
   - Assess balance on only one dimension
   
 - Challenges:
 
   - Choice of covariates
   - Model dependence (logit/probit)
   - Decision about 1-to-1 or 1-to-k or k-to-k matching
   - Assessing balance
   


---
template: outline

---
## How do we know when units are sufficiently matched?

 - Common covariate support
 
 - Covariate (mean) balance
 
 - Identical covariate distributions

???

Because there are unobserved covariates, we can never really know

This is also a problem for regression; there are always omitted variables

 1. We get common covariate support in any matching exercise
 
 2. Mean balance is something we have to test (exact matching ignores it)
 
   - We can do t-test, but do we need exactly identical means?
   - Do we look at the propensity score or individual covariates?
 
 3. Identical distributions

   - Is it necessary?
   - How do we test?


---
## Common covariate support


---
## Mean covariate balance


---
## Balanced covariate distributions


ADD MORE ON BALANCE TESTING




---
## Analysis

 - Matching is preprocessing
 
 - We still need to estimate a causal effect
 
 - We can use any method:
 
   - Difference-in-means
   - Weighted difference
   - Regression


---

ADD SOMETHING FROM STATA

STATA INTEGRATES MATCHING AND EFFECT ESTIMATION

IN LAB, WE'LL DO MATCHING BY HAND, THEN USE STATA'S BUILT-IN MATCHING PROCEDURES


---
## Activity

 - Divide into groups
 - Summarize one section to share with class

From Kam and Palmer:

   1) Argument (K&P pp.614-617)
   
   2) Methods (K&P pp.617-622)
   
   3) Results (K&P pp.622-626)

From Henderson and Chatfield:

   4) Method (H&C pp.649-652)
   
   5) Results (H&C pp.653-655)

 
???

Divide class into groups of ~3

Have each pair take a part of the two articles


---
## Preview

Tomorrow:

 - Matching in Stata
 
Next week:

 - Instrumental Variables (IV regression)
 
Week after next:

 - Missing data
 - Multiple imputation

???

IV focuses on estimating a Local Average Treatment Effect (LATE) by finding some real-world randomization to "instrument" for the causal variable

So-called "natural experiments", "regression discontinuity designs", and "interrupted time series" are all basically instrumental variables designs
