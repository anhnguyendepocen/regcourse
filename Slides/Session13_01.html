---
number: 13
title: Missing Data Imputation
day: 5 maj
type: lecture
layout: remark
---

# {{ page.title }} #
# {{ page.day }} #

???

Gelman and Hill (529-543)
King, Honaker, Joseph, and Scheve (2001)
Gilens (2001)
Simmons and Hopkins (2003)


---
name: outline
## Outline

 1. Instrumental Variables
 
 2. Missing data
 
 3. Single imputation methods
 
 4. Multiple imputation


---
## Missing data

 - What is it?
 
--

 - Why are data missing?
 
--

 - How often do we encounter missing data?

???

 - Most data we work with is organized, cleaned, etc.
 - In reality, data are never clean
   - We either have to make them or we get them from someone else
   - Not clean means badly coded and/or full of missing values
   - Coding is something we can handle, missing data is a big problem
 - What do we do when we have missing values?

---
## An Exercise

 - Find your 3rd assignment for this course
 
 - What was the sample size for your model?

 
---
## Wooldridge

 - Wooldridge only mentions missing data once:
 
> If the data are missing at random, then the size of the random sample available from the population is simply reduced. Although this makes the estimators less precise, it does not introduce any bias [...] There are ways to use the information on observations where only some variables are missing, but this is not often done in practice. The improvement in the estimators is usually slight, while the methods are somewhat complicated. In most cases, we just ignore the observations that have missing information.

.footnote[Wooldridge p.314]


---
## Mising Data In Practice

 - Ideally we don't have any missing data


---
## Effects of Missing Data

 1. Statistical efficiency
 
 2. Comparability of analyses
 
 3. Representativeness
 
 4. Scale construction
 
 5. Causal inference

???

Statistical efficiency (loss of information; effect on SEs)

Representativeness

Comparability of analyses

Scale construction (measurement error due to missingness)

Problem from a causal perspective: if missingness perfectly determined by a third variable, then we're estimating an implicit interaction and only seeing the effect for those with available data.



---
## Scaling
  - We haven't talked a lot about scaling in this class
  - Scaling is a good example of the problems associated with missing data
  - How do we build scales with missing values?


---
## Two Default Approaches

 1. Complete Case Analysis
 
   - i.e., listwise deletion
   
 2. Available Case Analysis
 
   - This is our default

---
## Available Case Analysis
  
 - Software default

 - Remove any observations with missing values for a given analysis
 
 - Benefits
    - Easy
    - Uses as much data as possible for any given analysis

 - Consequences
    - Loss of information
    - Analyses are not comparable to one another
    - Loss of sample representativeness?

???

Worst solution because it is the default. It's done unconsciously.

---
## Complete Case Analysis

 - Listwase or case deletion
 
 - Remove any observations with missing values before all analyses
 
 - Benefits
    - Easy
    - All analyses are comparable to one another

 - Consequences
    - Loss of information (more than available case)
    - Loss of sample representativeness?

???

Challenging because you need to identify all of your variables in advance of running any analyses, so you can drop those with missing values



---
## What can we do about missing data?

 - Use default strategies
 
 - Impute missing values once
 
 - Run analyses multiple times with different imputations

---
## Missing Data Assumptions

 1. **Missing Completely At Random** (MCAR)
 
 2. **Missing At Random** (MAR; Ignorable)
 
 3. **Nonignorable** (NI)

???

What do these assumptions mean?

Unfortunately, we can't really test them.

---
## MCAR

 1. Statistical efficiency
 
 2. Comparability of analyses
 
 3. Representativeness
 
 4. Scale construction
 
 5. Causal inference

???

Which of these is a problem?

---
## MAR/Ignorable

 1. Statistical efficiency
 
 2. Comparability of analyses
 
 3. Representativeness
 
 4. Scale construction
 
 5. Causal inference

???

Which of these is a problem?

---
## NI

 1. Statistical efficiency
 
 2. Comparability of analyses
 
 3. Representativeness
 
 4. Scale construction
 
 5. Causal inference

???

Which of these is a problem?


---
template: outline

---
## Single imputation

 - Impute missing values once
 
 - Benefits
   - Increases statistical efficiency
   - Comparable analyses
   - Easy (depends on technique)
   - Preserve representativeness?
 
 - Consequences
   - Bias (depends on technique)
   - Analyses do not reflect uncertainty due to missingness

   
???

In practice this is not all that common.

Consequence: replacement of missing values is potentially arbitrary; if missingness is ignorable (MAR), we need to model the missingness

- Modifies the features of univariate and multivariate distributions
      - Might change central tendency (mean, median), variance, etc.
    - How to calculate standard errors?

---
## Single Imputation Methods

 - Single value (e.g. zero, mean)

 - Random value
 
 - Inferred value

 - Hot deck
 
 - Regression
    
???

 1. Single value. What effect does this have on mean, variance, correlations? None, smaller, smaller
 
 2. Random value.
   - What distribution to draw from? Allowed values, allowed value weighted by their prevalence in the data? Continuous distribution?
   - What effect does this have on mean, variance, correlations? None, none, smaller
 
 3. Inferred value.
   - Use other data to infer a value. If income is missing, but R said they are unemployed, income is presumably zero.
   - What effect does this have on mean, variance, correlations? some, some, some
 
 4. Hot deck.
   - Use information in the dataset to infer values.
   - Based on cross-tabulation.
   - All cases that have the same set of observed values will have the same imputed value (no account for random variation)
   - What effect does this have on mean, variance, correlations? some, some, some
 
 5. Regression.
   - Use information in the dataset to infer values.
   - Uses standard regression techniques to make out-of-sample predictions. Fitted value from regression is used as imputed value.
   - All cases that have the same set of observed values will have the same imputed value (no account for random variation).
   - What effect does this have on mean, variance, correlations? some, some, some


---
template: outline


---
## Multiple Imputation (MI)

 1. Engage a single imputation method
 
 2. Repeat this method multiple times
 
 3. Run analysis on each imputed dataset
 
 4. Combine estimates and calculate combined variances

???

There are different algorithms for doing this:
 - King et al. have one
 - Gelman and Hill have another one
 - There are others

---
## Multiple Imputation (MI)

 - Benefits
   - Increases statistical efficiency
   - Account for uncertainty due to missingness
   - Comparable analyses
   - Better than single imputation if MAR
   
 - Consequences
   - Somewhat computationally expensive
   - Challenging with large datasets
   - Software may not handle all statistical tests in an MI framework


---
## Multiple Imputation (MI)

 - Still not very common in political science
 
 - Doesn't solve NI missingness


 

