\input{preamble}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
%\usetikzlibrary{decorations.pathreplacing}

\title{Interpretation of GLMs}

\date[]{April 21, 2015}

\begin{document}

\frame{\titlepage}

\frame{\tableofcontents}

\section{Essay 2}


\frame{
    \frametitle{Feedback on Essay 2}
    \begin{itemize}\itemsep1em
    \item Overall, these were ``OK'' but not great
    \item Issues mainly related to interpretation
    \item Other issues outlined in the email I sent
    \item Review a few of these issues quickly
    \end{itemize}
}


\frame{
    \frametitle{Interpretation}
    \begin{itemize}
    \item 
    \end{itemize}
}

% we are estimating coefficients for an equation

% using estimates, we can create predicted or fitted values based on input values

% graph these predicted values


% start graphing and interpretation activity (OLS only)

\frame{
    \frametitle{Graphing Activity}
    \begin{itemize}\itemsep2em
    \item It can be difficult to see regression without results without graphing them
    \item But, using Stata it can be unclear how the estimated regression equation conforms to particular graphical output
    \item So, you will practice graphing \textit{by hand}
    \item Do activities \# 1--3
    \end{itemize}
}




\begin{frame}[fragile]
    \frametitle{Representing Theory as Graph}
	\begin{center}
	\begin{tikzpicture}[>=latex',circ/.style={draw, shape=circle, node distance=5cm, line width=1.5pt}]
        \draw[->] (0,0) node[left] (X) {X} -- (5,0) node[right] (Y) {Y};
        \draw[->] (0,0) node[left] (X) {X} -- (2.5,0) node[right] (D) {D};
        \draw[->] (3.1,0) -- (5,0) node[right] (Y) {Y};
        \draw[->] (-3,4) node[above] (Z) {Z} -- (X);
        \draw[->] (Z) -- (Y);
        \draw[->] (5,2) node[above] (A) {A} -- (Y);
        \draw[->] (-2,0) node[left] (B) {B} -- (X);
        \draw[->] (X) -- (2,-2) node[right] (C) {C};
    \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{A ``Heuristic'' Graph}
	\begin{center}
	\begin{tikzpicture}[>=latex',circ/.style={draw, shape=circle, node distance=5cm, line width=1.5pt}]
        \draw[->] (0,0) node[left] (X) {X} -- (5,0) node[right] (Y) {Y};
        \draw[->] (0,0) node[left] (X) {X} -- (2.5,0) node[right] (D) {D};
        \draw[->] (3.1,0) -- (5,0) node[right] (Y) {Y};
        \draw[->] (-3,4) node[above] (Z) {Z} -- (X);
        \draw[->] (Z) -- (Y);
        \draw[->] (2.5,-1) node[below] (M) {M} -- (D);
    \end{tikzpicture}
    \end{center}
\end{frame}
% causal graphs vs. linear path models vs. ``heuristic'' theoretical drawings







\section{Review GLMs and MLE}


\frame{
    \frametitle{Non-continuous Outcomes}
    \begin{enumerate}\itemsep1em
    \item Why shouldn't we use OLS for a non-continuous outcome variable?
    \item<2-> What do we do instead? 
    \end{enumerate}
}


\frame{
    \frametitle{Regression on a Latent Variable}
    \begin{itemize}\itemsep1em
    \item Consider a binary outcome $y$ (e.g., voting)
    \item OLS provides a nonsensical fit to the outcome
    \item Think about the problem as a ``latent'' outcome ($y\star$) that manifests in two observed categories
        \begin{itemize}
        \item As $y\star$ increases, $Pr(Y=1) \rightarrow 1$
        \item As $y\star$ decreases, $Pr(Y=1) \rightarrow 0$
        \end{itemize}
    \item We do not observe $y\star$, only $y$
    \end{itemize}
}

\frame{
    \frametitle{Estimation in GLM}
    \begin{itemize}\itemsep1em
    \item In OLS, we estimate:\\
        $\hat{y} = \beta_0 + \beta_1 x + e$
    \item This is a prediction of the conditional mean of $y$
    \item In a GLM, we estimate:\\
        $\hat{y}\star = \beta_0 + \beta_1 x + e$\\
    where $y\star$ is a transformation of $y$
    \item This is also a prediction of the conditional mean of $y$
    \item<2-> How do we perform the transformation of $y$ to $y\star$?
    \end{itemize}
}


\frame{
    \frametitle{Link Function}
    \begin{enumerate}\itemsep2em
    \item \textbf{Link function}: $g()$
        \begin{itemize}
        \item Transforms $y$ to $y\star$
        \end{itemize}
    \item \textbf{Inverse link function}: $g^{-1}()$
        \begin{itemize}
        \item Transforms $y\star$ back to $y$
        \end{itemize}
    \item<2->
    \end{enumerate}
}








\frame{
    \frametitle{The Likelihood Function}
    \begin{itemize}\itemsep1em
    \item Takes possible population parameters as inputs
    \item Give a \textit{probability} of seeing each observation in our sample data given that input parameters
    \item Combine those probabilities (i.e., likelihoods)
        \begin{itemize}
        \item Multiply the likelihoods
        \item Add the log-likelihoods
        \end{itemize}
    \item Repeat process for various parameter values
    \item We pick the best guess from all of those that we test
    \end{itemize}
}



% assumptions




\section{Interpreting GLMs}

% activity discussing one of the assigned articles (Hobolt maybe?)


\frame{
    \frametitle{Interpretation}
    \begin{itemize}\itemsep1em
    \item Recall the Hobolt article from last week
    \item What is her research question? What is her theory?
    \item What is the test of that theory?
    \item How does she interpret the results?
    \end{itemize}

}

% coefficients in logit and probit

\frame{
    \frametitle{Coefficients}
    \begin{itemize}\itemsep1em
    \item Coefficients express effect of $x$ on $y\star$
    \item In logistic regression, this is a statement about the odds-ratio:
        $\hat{\beta} = \dfrac{\frac{p_1}1-p_1}}{\frac{p_0}{1-p_0}}$
    \item Coefficients are hard to interpret \textit{substantively}
    \item Statistical significance works more or less as in OLS
    \end{itemize}
}

\frame{
    \frametitle{Predicted Outcomes}

}

\frame{
    \frametitle{Predicted Probabilities}
    \begin{itemize}
    \item What is the probability that $y_i=1|\boldsymbol{x}_i$?
    \end{itemize}
}



\frame{
    \frametitle{Marginal Effects}

}


\frame{
    \frametitle{Graphing Activity (cont.)}
    \begin{itemize}\itemsep1em
    \item Do graphing activity \# 4
    \end{itemize}
}

\frame{
    \frametitle{Three Types of Marginal Effects}
    \begin{enumerate}\itemsep1em
    \item Marginal Effects at the Mean (MEMs)
    \item Marginal Effects at Representative Values (MERs)
    \item Average Marginal Effects (AMEs)
    \end{enumerate}
}


\frame{
    \frametitle{MEM}
    \begin{itemize}\itemsep1em
    \item 
    \end{itemize}
}

\frame{
    \frametitle{MERs}
    \begin{itemize}\itemsep1em
    \item
    \end{itemize}
}

\frame{
    \frametitle{AMEs}
    \begin{itemize}\itemsep1em
    \item The MEM/AME distinction
    \end{itemize}
}


\frame{
    \frametitle{Discrete Changes}
    \begin{itemize}\itemsep1em
    \item Marginal effects are \textit{instantaneous changes}
    \item This makes sense for continuous independent variables
    \item For categorical (factor) variables, we often instead calculate a discrete change
        \begin{itemize}
        \item $Pr(y=1|x=1) - Pr(Y=1|x=0)$
        \item Marginal effect and discrete change are the same in OLS
        \end{itemize}
    \end{itemize}
}

\frame{
    \frametitle{Graphing Activity (cont.)}
    \begin{itemize}\itemsep1em
    \item Do graphing activity \# 5
    \end{itemize}
}


\frame{
    \frametitle{Interaction Terms}
    \begin{itemize}\itemsep1em
    \item Due to the link function transformation, the marginal effect of $x$ depends on the value of $x$ and all other covariates
    \item This creates \textit{implicit} interactions
    \item We still have to include \textit{explicit} interaction terms to estimate heterogeneous effects (i.e., effect moderation)
    \end{itemize}
}


\frame{
    \frametitle{Graphing Activity (cont.)}
    \begin{itemize}\itemsep1em
    \item Do graphing activity \# 6
    \end{itemize}
}



\section{Model Specification}

\frame{
    \frametitle{Model Specification}
    \begin{enumerate}\itemsep1em
    \item<1-> Complete set of conditioning variables
    \item<2-> Correctly specified model
    \item<3-> Choice of error distribution
    \item<4-> Link function
    \end{enumerate}
}

\frame{
    \frametitle{Common Link Functions}
}

% logit versus probit

% decision is based on expected distribution of the error term
% recall in OLS, we don't make any assumptions about the error term to obtain coefficient estimates, but we do make those assumptions to estimate the standard errors
% in MLE, we need to make an assumption about the distribution of the error term in order to estimate coefficients.
% for a continuous DV, we typically assume Normally distributed errors; for binary outcomes, it is conventional to assume a logistic distribution (logit) or a Normal distribution (probit)
% logit and probit both introduce a symmetrical relationship between covariates and the probability that y = 1
% other models do not assume that symmetry (log-log; complementary log-log; etc.)

% elaborating beyond binary outcomes (ordered, categorical, count)

% robust SEs - don't necessarily make sense in a GLM context





\appendix
\frame{}

\end{document}
