\input{preamble}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathreplacing}

\title{Interpretation of GLMs}

\date[]{April 21, 2015}

\begin{document}

\frame{\titlepage}

\frame{\tableofcontents}

\section{Essay 2}


% causal graphs vs. linear path models vs. ``heuristic'' theoretical drawings




% we are estimating coefficients for an equation

% using estimates, we can create predicted or fitted values based on input values

% graph these predicted values


% start graphing and interpretation activity (OLS only)




\section{Review GLMs and MLE}


% ways of thinking about this

% transforming outcome to a continuous scale so that we can estimate effects

% latent utility

% latent variable

% 


\frame{

\frametitle{The Likelihood Function}
 
 - Takes possible population parameters as inputs
   - i.e., guess at `\(\boldsymbol{\beta}\)`
   
 - Give a *likelihood* of seeing our sample data given that input
   - i.e., the probability of observing the data given the parameter guesses
 
 - We pick best guess
   - i.e., the guesses at `\(\boldsymbol{\beta}\)` that yield the *maximum* likelihood

}


\frame{

\frametitle{Maximizing the likelihood}

How do we find the maximum likelihood (and thus our estimated `\(\beta\)`'s)?

We guess! Repeatedly!
  
  - Start with some kind of guess and calculate the likelihood
  
  - Make better and better guesses, until the likelihood doesn't get marginally higher

}




% assumptions




\section{Interpreting GLMs}

% activity discussing one of the assigned articles (Hobolt maybe?)




% coefficients in logit and probit

% average marginal effects
% marginal effects at the mean
% marginal effects at representative values

% latent scale versus probability scale

\frame{

What is the probability that $y_i=1|\boldsymbol{x}_i$ ?

}


% marginal effects (slope of the response curve)
% discrete changes
%% there is no difference in linear additive (i.e., many OLS) models because the effect of X is constant across levels of X



% graphing predicted probabilities
% graphing marginal effects


% rest of graphing activity


% interaction terms


\section{Model Specification}

% logit versus probit

% decision is based on expected distribution of the error term
% recall in OLS, we don't make any assumptions about the error term to obtain coefficient estimates, but we do make those assumptions to estimate the standard errors
% in MLE, we need to make an assumption about the distribution of the error term in order to estimate coefficients.
% for a continuous DV, we typically assume Normally distributed errors; for binary outcomes, it is conventional to assume a logistic distribution (logit) or a Normal distribution (probit)
% logit and probit both introduce a symmetrical relationship between covariates and the probability that y = 1
% other models do not assume that symmetry (log-log; complementary log-log; etc.)

% elaborating beyond binary outcomes (ordered, categorical, count)

% robust SEs - don't necessarily make sense in a GLM context





\appendix
\frame{}

\end{document}
